Okay, hier ist die ausf√ºhrliche Dokumentation und Bedienungsanleitung als `README.md` im Markdown-Format mit Symbolen, wie gew√ºnscht:

# üöÄ WebCrawler-Pro ‚Äì Dokumentation und Bedienungsanleitung

## 1. Einleitung

**WebCrawler-Pro** ist eine hochentwickelte und robuste Anwendung f√ºr das automatisierte Web-Scraping. Es erm√∂glicht das effiziente Erfassen von Webinhalten, von einfachem Text bis hin zu komplexen HTML-Strukturen, und bietet dabei eine Vielzahl von Funktionen f√ºr die Datenextraktion, -verarbeitung und -verwaltung. WebCrawler-Pro zeichnet sich durch seine Flexibilit√§t in der Nutzung aus ‚Äì ob √ºber die Kommandozeile f√ºr schnelle Aufgaben, als programmatische API f√ºr Integrationen oder √ºber eine intuitive Streamlit-Admin-Oberfl√§che f√ºr umfassende Task-Verwaltung.

**üéØ Hauptmerkmale und Verbesserungen:**

*   **Integrierter API-Key Generator:** Beinhaltet ein `key_generator.py` Skript zum einfachen Generieren und Verwalten von API-Keys.
*   **Datenbankunterst√ºtzung mit SQLite:** Verwendet eine leichtgewichtige SQLite-Datenbank f√ºr die Speicherung der gescrapten Inhalte und Task-Konfigurationen.
*   **Erweiterte Task-Verwaltung:**  Umfassendes CRUD-Management f√ºr geplante Web-Scraping-Tasks √ºber API und Streamlit UI. Speichern von Task-Konfigurationen und Status in der Datenbank.
*   **Verbesserte Robustheit und Leistung:**
    *   **Asynchrone Web-Requests (aiohttp):**  Prim√§rer Web-Abrufmechanismus f√ºr hohe Performance und Effizienz.
    *   **Selenium Fallback:**  Automatischer Fallback auf Selenium f√ºr dynamische Webseiten mit JavaScript-Rendering.
    *   **Erh√∂hte Wiederholungsversuche & Verz√∂gerungen:**  Anpassbare `max_retries` (Standard: 5) und `retry_delay` (Standard: 3 Sekunden) f√ºr zuverl√§ssigeren Webseitenabruf.
    *   **Effizientes Caching:**  Zwischenspeicherung von Webseiteninhalten zur Reduzierung von redundanten Anfragen und zur Performance-Steigerung (anpassbare `cache_expiry_seconds`, Standard: 10 Minuten).
    *   **Rate Limiting:**  Schutz der API durch konfigurierbares Rate Limiting (Standard: 20 Anfragen pro Minute), um √úberlastungen zu vermeiden.
*   **Umfassende API mit API-Key-Authentifizierung:**  Gesicherte Web-API f√ºr programmatischen Zugriff auf alle Kernfunktionen, inklusive Task-Management und Web-Scraping. API-Key-Authentifizierung und Rate Limiting f√ºr Sicherheit und Stabilit√§t.
*   **Intuitive Streamlit Admin-Oberfl√§che:**  Web-basierte UI zur einfachen Verwaltung und √úberwachung von geplanten Tasks, inklusive Task-Erstellung, -Bearbeitung, -L√∂schung und manueller Ausf√ºhrung. Zus√§tzlich eine Datenbankbrowser-Oberfl√§che f√ºr die Inhaltsrecherche.
*   **Erweiterte Datenextraktion:**
    *   **CSS-Selektoren mit Sicherheitsvalidierung:**  Flexible Extraktion strukturierter Daten mittels CSS-Selektoren, inklusive konfigurierbarer Datentypen und Bereinigungsfunktionen. Sicherheitspr√ºfungen zum Schutz vor CSS-Injection.
    *   **Keyword-Extraktion:**  Automatische Keyword-Analyse extrahierter Textinhalte.
    *   **Benutzerdefinierte Datenverarbeitung:**  Option zur Integration eigener Python-Funktionen zur individuellen Verarbeitung der gescrapten Daten.
*   **Verbesserte Fehlerbehandlung und Logging:**  Detaillierte Protokollierung mit konfigurierbaren Log-Leveln (DEBUG, INFO, WARNING, ERROR, CRITICAL) f√ºr effektive Fehleranalyse und -behebung. Datenbank-Transaktionen f√ºr Datensicherheit.
*   **Sicherheitsfokus:**  API-Key-Authentifizierung, Rate Limiting, CSS-Selektor-Validierung, Pfadvalidierung f√ºr Processing-Funktionen, Datenbank-Transaktionen.
*   **Flexibilit√§t in der Nutzung:**  Kommandozeile, Web-API und Streamlit Admin-Oberfl√§che f√ºr unterschiedliche Anwendungsf√§lle und Benutzerpr√§ferenzen.

**üéØ Zweck:**

*   ‚úÖ Automatisierte Datenerfassung aus dem Web f√ºr anspruchsvolle Forschungs- und Analysezwecke, erweiterte Marktanalysen, umfassende Content-Aggregation und mehr.
*   ‚è±Ô∏è Zuverl√§ssige und regelm√§√üige √úberwachung von Webseiteninhalten durch flexible, zeitgesteuerte Tasks mit robuster Task-Verwaltung.
*   üåê Professionelle API-Bereitstellung f√ºr den sicheren und effizienten Zugriff auf Web-Scraping-Funktionalit√§ten in Enterprise-Anwendungen und komplexen Systemen.
*   üñ•Ô∏è Benutzerfreundliche und effiziente Verwaltung und √úberwachung umfangreicher geplanter Web-Scraping-Aufgaben √ºber eine moderne Web-Oberfl√§che mit integriertem Datenbankbrowser und API-Key Generator.

**üë• Zielgruppe:**

*   üìä Fortgeschrittene Datenanalysten und Wissenschaftler, die hochvolumige und komplexe Webinhalte f√ºr tiefgehende Analysen und Forschungsprojekte ben√∂tigen.
*   üßë‚Äçüíª Softwareentwickler und Systemarchitekten, die robuste und skalierbare Web-Scraping-Funktionalit√§ten in ihre professionellen Anwendungen und Services integrieren m√∂chten.
*   üì¢ Enterprise Content-Manager und Marketingstrategen, die umfassende Web-Intelligence-L√∂sungen zur Wettbewerbsanalyse und Marktbeobachtung ben√∂tigen.
*   üõ†Ô∏è Technische Experten und DevOps-Teams, die eine hochflexible, konfigurierbare und sichere L√∂sung f√ºr Enterprise-Web-Scraping-Anforderungen suchen.

## 2. Systemanforderungen

Um WebCrawler-Pro in vollem Umfang nutzen zu k√∂nnen, sind folgende Systemvoraussetzungen empfehlenswert:

**üíª Hardware:**

*   **Prozessor:** Intel Core i5 oder vergleichbarer Mehrkernprozessor (empfohlen: Intel Core i7 oder Xeon f√ºr hochvolumige Scraping-Aufgaben und API-Last)
*   **Arbeitsspeicher:** Mindestens 8 GB RAM (empfohlen: 16 GB RAM oder mehr, insbesondere f√ºr parallele Scraping-Prozesse, API-Betrieb unter Last und Datenbankoperationen)
*   **Festplattenspeicher:** Mindestens 5 GB freier, schneller Festplattenspeicher (SSD empfohlen) f√ºr die Programminstallation, Datenbank und Caching (der ben√∂tigte Speicherplatz kann je nach Umfang der gescrapten Daten und Cache-Gr√∂√üe variieren)

**üíæ Software:**

*   **Betriebssystem:** Windows 10/11 oder Server-√Ñquivalent, macOS 11 (Big Sur) oder h√∂her, Linux (getestet auf aktuellen Ubuntu LTS und CentOS/RHEL)
*   **Python:** Python 3.9, 3.10 oder 3.11 (empfohlen: Python 3.10 f√ºr optimale Performance und Kompatibilit√§t). Stellen Sie sicher, dass Python und `pip` im Systempfad verf√ºgbar sind.
*   **Webbrowser:** Google Chrome (neueste stabile Version f√ºr optimale Selenium-Kompatibilit√§t und Performance).
*   **ChromeDriver:** Aktuelle ChromeDriver-Version, automatisch verwaltet durch `webdriver-manager`.
*   **Datenbank-System (optional):**
    *   **SQLite (Standard):** F√ºr kleinere bis mittlere Scraping-Projekte und Einzelplatzinstallationen ausreichend.
    *   **PostgreSQL oder MySQL:**  Empfohlen f√ºr Enterprise-Umgebungen, hochvolumige Daten, API-Betrieb unter Last und verbesserte Datenbank-Performance und -Zuverl√§ssigkeit. Datenbankserver muss separat installiert und konfiguriert werden. **Hinweis:** PostgreSQL und MySQL-Unterst√ºtzung ist aktuell nur rudiment√§r implementiert.

**üêç Python Bibliotheken:**

Die folgenden Python-Bibliotheken sind zwingend erforderlich und werden typischerweise bei der Installation automatisch durch `pip` installiert:

*   `streamlit` (f√ºr Admin- und Datenbankbrowser-Oberfl√§che)
*   `pandas` (f√ºr Datenmanipulation und -darstellung)
*   `beautifulsoup4` (f√ºr HTML-Parsing)
*   `selenium` (f√ºr dynamisches Web-Scraping und JavaScript-Rendering)
*   `webdriver_manager` (f√ºr automatische ChromeDriver-Verwaltung)
*   `flask` (f√ºr Web-API)
*   `sqlite3` (f√ºr SQLite-Datenbank)
*   `nltk` (f√ºr Keyword-Extraktion)
*   `pyyaml` (f√ºr Konfigurationsmanagement)
*   `pydantic` (f√ºr Datenvalidierung)
*   `python-dotenv` (f√ºr Umgebungsvariablen-Management)
*   `aiohttp` (f√ºr asynchrone HTTP-Requests)
*   `chardet` (f√ºr Zeichenkodierungserkennung)
*   `mimetypes` (f√ºr MIME-Type-Erkennung)

## 3. Installation

Befolgen Sie diese detaillierten Schritte, um WebCrawler-Pro optimal zu installieren und f√ºr Ihre Anforderungen zu konfigurieren:

**Schritt 1: Python installieren (und `venv`, `pip` sicherstellen)**

Laden Sie eine der empfohlenen Python-Versionen (3.9, 3.10 oder 3.11) von der offiziellen Python-Webseite ([https://www.python.org/downloads/](https://www.python.org/downloads/)) herunter und installieren Sie diese.

*   **Wichtig:** Aktivieren Sie unbedingt die Option "Add Python to PATH" w√§hrend der Installation, damit Python und `pip` √ºber die Kommandozeile zug√§nglich sind.
*   **Virtuelle Umgebung (`venv`) und Paketmanager (`pip`):**  Moderne Python-Installationen beinhalten in der Regel bereits `venv` (f√ºr virtuelle Umgebungen) und `pip` (Paketmanager). Stellen Sie sicher, dass diese verf√ºgbar sind, indem Sie `python -m venv --version` und `pip --version` in Ihrem Terminal ausf√ºhren.

**Schritt 2: Projektdateien herunterladen und entpacken**

Laden Sie die WebCrawler-Pro Projektdateien als ZIP-Archiv herunter und entpacken Sie dieses in einen lokalen Ordner Ihrer Wahl (z.B. `C:\webcrawler-pro` unter Windows oder `/home/user/webcrawler-pro` unter Linux/macOS).

**Schritt 3: Virtuelle Umgebung erstellen (stark empfohlen)**

Wechseln Sie im Terminal/Eingabeaufforderung in den entpackten Projektordner. Erstellen Sie eine isolierte virtuelle Umgebung, um die Projekt-Abh√§ngigkeiten von Ihrem globalen Python-System zu trennen:

```bash
python -m venv venv
```

**Schritt 4: Virtuelle Umgebung aktivieren**

Aktivieren Sie die neu erstellte virtuelle Umgebung, um nachfolgende `pip`-Befehle innerhalb dieser isolierten Umgebung auszuf√ºhren:

*   **Windows (Eingabeaufforderung):** `venv\Scripts\activate`
*   **Windows (PowerShell):** `venv\Scripts\Activate.ps1`
*   **macOS/Linux (Bash/Zsh):** `source venv/bin/activate`

    *   **Hinweis:** Nach der Aktivierung sollte der Name der virtuellen Umgebung `(venv)` am Anfang Ihrer Terminal-Prompt angezeigt werden.

**Schritt 5: Python-Bibliotheken installieren**

Installieren Sie alle erforderlichen Python-Bibliotheken und deren Abh√§ngigkeiten aus der `requirements.txt` Datei des Projekts:

```bash
pip install -r requirements.txt
```

    *   **Alternativ (falls keine `requirements.txt` vorhanden):**
        ```bash
        pip install streamlit pandas beautifulsoup4 selenium webdriver-manager flask sqlite3 nltk pyyaml pydantic python-dotenv aiohttp chardet mimetypes
        ```

**Schritt 6: Konfigurationsdatei `config.yaml` anpassen (optional, aber empfohlen)**

√ñffnen Sie die `config.yaml` Datei im Projektordner mit einem Texteditor. √úberpr√ºfen und passen Sie die folgenden wichtigen Konfigurationsoptionen nach Bedarf an:

*   **`database_file`:**  Pfad zur SQLite-Datenbankdatei. Standardm√§√üig `webdata.db`.
*   **`database_type`:**  Datenbanktyp. Standardm√§√üig `sqlite`. Aktuell ist nur SQLite vollst√§ndig unterst√ºtzt. Optionen `postgresql` und `mysql` sind rudiment√§r vorhanden, aber noch nicht vollst√§ndig funktionsf√§hig.
*   **`schedule_config_file`:** Pfad zur Datei f√ºr die Task-Zeitplankonfiguration (JSON-Format). Standardm√§√üig `scheduled_tasks.json`.
*   **`api_keys`:**  Liste der API-Keys f√ºr die Authentifizierung.  Sicherer ist die Verwendung von Umgebungsvariablen (siehe Abschnitt 3.1).
*   **`rate_limit_requests_per_minute`:**  Maximale Anzahl API-Anfragen pro Minute. Anpassen nach Bedarf.
*   **`cache_expiry_seconds`:**  G√ºltigkeitsdauer des Webseiten-Caches in Sekunden. Standardm√§√üig 600 Sekunden (10 Minuten).
*   **`selenium_config`:**  Konfiguration f√ºr Selenium (z.B. `headless`, `user_agent`).
*   **`allowed_css_properties`:**  Whitelist f√ºr erlaubte CSS-Eigenschaften in CSS-Selektoren (Sicherheitsma√ünahme).
*   **`processing_functions_dir`:**  Verzeichnis f√ºr benutzerdefinierte Processing-Funktionen.
*   **`log_level`:**  Log-Level f√ºr die Anwendung (z.B. `INFO`, `DEBUG`, `WARNING`).

**Schritt 7: API-Keys generieren (optional, aber empfohlen)**

F√ºr den Zugriff auf die Web-API und die Streamlit Admin-Oberfl√§che ben√∂tigen Sie API-Keys. Verwenden Sie das mitgelieferte `key_generator.py` Skript, um API-Keys zu generieren und in der `.env`-Datei und `config.yaml` zu speichern:

1.  **Terminal √∂ffnen und zum Projektordner navigieren:**  √ñffnen Sie Ihr Terminal/Eingabeaufforderung und wechseln Sie in das Hauptverzeichnis des WebCrawler-Pro Projekts.
2.  **Virtuelle Umgebung aktivieren (falls verwendet):**  Aktivieren Sie die virtuelle Umgebung, falls Sie eine erstellt haben (siehe Schritt 4 der Installation).
3.  **`key_generator.py` ausf√ºhren:** F√ºhren Sie das Key-Generator-Skript aus, um standardm√§√üig 3 API-Keys zu generieren und zu speichern:

    ```bash
    python key_generator.py
    ```

    *   Sie k√∂nnen die Anzahl der zu generierenden Keys optional anpassen, indem Sie den Parameter `num_keys` im Skript √§ndern.
    *   Die generierten API-Keys werden sowohl in der `.env`-Datei (f√ºr Umgebungsvariablen) als auch in der `config.yaml` unter `api_keys` gespeichert.

**Schritt 8: ChromeDriver Installation (automatisch durch `webdriver-manager`)**

ChromeDriver wird beim ersten Start von Selenium automatisch durch `webdriver-manager` heruntergeladen und installiert, passend zu Ihrer installierten Chrome-Version. Sie m√ºssen ChromeDriver nicht manuell installieren.

**Schritt 9: Installation abschlie√üen**

WebCrawler-Pro ist nun erfolgreich installiert und konfiguriert. Sie k√∂nnen das Programm im Kommandozeilenmodus, als Web-API oder √ºber die Streamlit Admin-Oberfl√§che starten.

### 3.1 API-Keys sicher √ºber Umgebungsvariablen konfigurieren üîë

F√ºr erh√∂hte Sicherheit und einfachere Verwaltung empfiehlt es sich, API-Keys √ºber Umgebungsvariablen zu konfigurieren, anstatt sie direkt in der `config.yaml` Datei zu hinterlegen. Das `key_generator.py` Skript (Schritt 7 der Installation) unterst√ºtzt dies automatisch und speichert die Keys sowohl in `.env` als auch in `config.yaml`.

**Manuelle Konfiguration √ºber `.env` Datei (Alternative):**

1.  **`.env` Datei erstellen:**  Erstellen Sie im Hauptprojektordner eine neue Datei namens `.env` (ohne Dateiendung).
2.  **API-Keys in `.env` eintragen:** F√ºgen Sie Ihre API-Keys in die `.env` Datei ein, wobei jeder Key in einer separaten Zeile im Format `API_KEY_NUMMER=IhrAPIKey` definiert wird:

    ```dotenv
    API_KEY_1=IhrErsterAPIKey
    API_KEY_2=IhrZweiterAPIKey
    API_KEY_3=IhrDritterAPIKey
    # ... weitere API-Keys bei Bedarf
    ```

    *   **Ersetzen Sie `IhrErsterAPIKey`, `IhrZweiterAPIKey`, `IhrDritterAPIKey` durch Ihre tats√§chlichen API-Keys.**
3.  **Umgebungsvariablen laden:**  WebCrawler-Pro l√§dt beim Start automatisch die in der `.env` Datei definierten Umgebungsvariablen und verwendet diese f√ºr die API-Key-Authentifizierung.

### 3.2 Fehlerbehebung bei der Installation und h√§ufige Probleme üõ†Ô∏è

*   **Fehlende Python-Bibliotheken:** Stellen Sie sicher, dass Sie alle Bibliotheken gem√§√ü Schritt 5 installiert haben (`pip install -r requirements.txt` oder manuell). √úberpr√ºfen Sie die Fehlermeldungen im Terminal auf fehlende Bibliotheken.
*   **ChromeDriver-Probleme:** Wenn Selenium-Operationen fehlschlagen, stellen Sie sicher, dass Sie eine kompatible Version von Google Chrome installiert haben. `webdriver-manager` sollte ChromeDriver automatisch verwalten. √úberpr√ºfen Sie die Logs auf ChromeDriver-bezogene Fehlermeldungen.
*   **Datenbankverbindungsfehler (PostgreSQL/MySQL):**  Vergewissern Sie sich, dass der PostgreSQL- oder MySQL-Datenbankserver l√§uft und die in `database_url` angegebenen Verbindungsinformationen korrekt sind (Hostname, Port, Benutzername, Passwort, Datenbankname). Testen Sie die Datenbankverbindung separat mit einem Datenbank-Client-Tool. **Hinweis:** Aktuell wird nur SQLite vollst√§ndig unterst√ºtzt.
*   **Berechtigungsfehler:**  Stellen Sie sicher, dass das Python-Skript Schreibrechte im Projektordner hat (z.B. f√ºr die SQLite-Datenbankdatei oder Logdateien).
*   **Portkonflikte (API/Streamlit):**  Wenn die API oder Streamlit Admin-Oberfl√§che nicht starten, k√∂nnte der Standardport (API: 5000, Streamlit: 8501) bereits von einer anderen Anwendung verwendet werden. Sie k√∂nnen versuchen, die Ports in der Anwendungskonfiguration zu √§ndern (falls konfigurierbar) oder die konkurrierende Anwendung zu stoppen.
*   **`config.yaml` Fehler:**  √úberpr√ºfen Sie die `config.yaml` Datei auf Syntaxfehler (z.B. falsche YAML-Formatierung). Verwenden Sie einen YAML-Validator online, um die Syntax zu pr√ºfen.
*   **Log-Dateien pr√ºfen:**  √úberpr√ºfen Sie die Log-Dateien (falls aktiviert) auf detailliertere Fehlermeldungen und Hinweise zur Fehlerursache. Konfigurieren Sie den `log_level` in `config.yaml` auf `DEBUG` f√ºr detailliertere Protokollierung bei Problemen.

F√ºr weitere detaillierte Fehlerbehebungshinweise und spezifische Probleme konsultieren Sie bitte die vollst√§ndige Dokumentation (falls vorhanden) oder die Online-Community/Support-Kan√§le von WebCrawler-Pro.

## 4. Benutzerf√ºhrung

WebCrawler-Pro bietet drei Hauptnutzungsmodi, die jeweils f√ºr unterschiedliche Szenarien und Benutzerpr√§ferenzen optimiert sind: Kommandozeilenmodus, Web-API und Streamlit Admin-Oberfl√§che.

### 4.1 Kommandozeilenmodus ‚å®Ô∏è ‚Äì F√ºr schnelle, einmalige Aufgaben

Der Kommandozeilenmodus eignet sich ideal f√ºr das schnelle Ausf√ºhren von Web-Scraping-Aufgaben, insbesondere f√ºr einmalige Abrufe oder Testzwecke.

1.  **Terminal √∂ffnen und zum Projektordner navigieren:**  √ñffnen Sie Ihr Terminal/Eingabeaufforderung und wechseln Sie in das Hauptverzeichnis des WebCrawler-Pro Projekts.
2.  **Virtuelle Umgebung aktivieren (falls verwendet):**  Aktivieren Sie die virtuelle Umgebung, falls Sie eine erstellt haben (siehe Schritt 4 der Installation).
3.  **Kommando ausf√ºhren:**  Verwenden Sie den `python app.py` Befehl gefolgt von der URL und optionalen Argumenten.

**Grundlegende Nutzung (URL als erstes Argument):**

```bash
python app.py <URL_der_zu_scrapen_Webseite>
```

    *   **Beispiel:** `python app.py https://www.example.com`
        *   Dieser Befehl startet den Web-Scraping-Prozess f√ºr die angegebene URL, extrahiert Inhalte und speichert diese (standardm√§√üig in der Datenbank).

**Optionale Argumente (nach `app.py` und URL):**

| Argument               | Kurzbeschreibung                                                                                                                                         | Beispiel                                                                                                   |
| :--------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------- |
| `--text`               | Speichert nur den extrahierten Textinhalt (anstatt des gesamten HTML-Codes) in der Datenbank und optional in einer Datei.                                 | `python app.py --text https://www.example.com`                                                           |
| `--save-file`          | Speichert den extrahierten Inhalt (HTML oder Text, abh√§ngig von `--text`) zus√§tzlich zur Datenbank in einer Textdatei im Projektverzeichnis.            | `python app.py --save-file https://www.example.com`                                                      |
| `--stopwords "<Liste>"` | Verwendet eine kommagetrennte Liste von zus√§tzlichen Stopw√∂rtern f√ºr die Keyword-Extraktion. Ersetzt `<Liste>` durch Ihre Stopw√∂rter in Anf√ºhrungszeichen. | `python app.py --stopwords "zus√§tzlich,weiteres,unwichtig" https://www.example.com`                      |
| `--css-selectors '<JSON>'` | Verwendet einen JSON-String zur Definition von CSS-Selektoren f√ºr die strukturierte Datenextraktion. Ersetzt `<JSON>` durch Ihren JSON-String.                                      | `python app.py --css-selectors '{"title": "h1", "paragraph": ".article-text p"}' https://www.example.com` |
| `--processing-function <Pfad>` | Verwendet eine benutzerdefinierte Python-Funktion zur Datenverarbeitung. Ersetzt `<Pfad>` durch den relativen Pfad zur Python-Datei mit der Funktion `process_data(data)`.                     | `python app.py --processing-function custom_processing.py https://www.example.com`                       |
| `--api`                | Startet WebCrawler-Pro im Web-API-Modus.                                                                                                               | `python app.py --api`                                                                                     |
| `--streamlit`          | Startet die Streamlit Admin-Oberfl√§che im Webbrowser.                                                                                                   | `python app.py --streamlit`                                                                               |
| `--db-browser`         | Startet die Streamlit Datenbankbrowser-Oberfl√§che im Webbrowser.                                                                                         | `python app.py --db-browser`                                                                            |
| *(keine URL, keine Option)* | Startet WebCrawler-Pro im Scheduled Mode (geplante Tasks aus Datenbank werden ausgef√ºhrt).                                                              | `python app.py`                                                                                           |

**Beispiele f√ºr Kommandozeilenbefehle:**

*   **Nur Text von einer Webseite extrahieren und in der Datenbank speichern:**
    ```bash
    python app.py --text https://www.example.com/artikel
    ```

*   **HTML-Inhalt extrahieren, in Datenbank speichern und zus√§tzlich als Datei speichern:**
    ```bash
    python app.py --save-file https://www.example.com/produktseite
    ```

*   **Keywords extrahieren mit zus√§tzlichen Stopw√∂rtern:**
    ```bash
    python app.py --stopwords "sonderangebot,rabatt,aktion" https://www.example.com/angebote
    ```

*   **Strukturierte Daten mit CSS-Selektoren extrahieren (Titel und Artikeltext):**
    ```bash
    python app.py --css-selectors '{"artikel_titel": "h1.title", "artikel_inhalt": ".article-body"}' https://www.example.com/news/artikel123
    ```

*   **Daten mit einer benutzerdefinierten Python-Funktion verarbeiten:**
    ```bash
    python app.py --processing-function benutzerdefinierte_verarbeitung.py https://www.example.com/daten
    ```

*   **WebCrawler-Pro im API-Modus starten:**
    ```bash
    python app.py --api
    ```

*   **Streamlit Admin-Oberfl√§che starten:**
    ```bash
    python app.py --streamlit
    ```

*   **Streamlit Datenbankbrowser starten:**
    ```bash
    python app.py --db-browser
    ```

*   **Scheduled Mode starten (geplante Tasks ausf√ºhren):**
    ```bash
    python app.py
    ```

### 4.3 Streamlit Admin-Oberfl√§che üñ•Ô∏è ‚Äì Task- und Datenbankverwaltung

Die Streamlit Admin-Oberfl√§che bietet eine benutzerfreundliche Weboberfl√§che zur Verwaltung von geplanten Tasks und zur Datenbankrecherche.

1.  **Admin-Oberfl√§che starten:** Starten Sie die Streamlit Admin-Oberfl√§che √ºber die Kommandozeile:

    ```bash
    python app.py --streamlit
    ```

    *   Die Admin-Oberfl√§che ist nun unter der Adresse `http://localhost:8501` erreichbar.

2.  **API-Key eingeben:**  Geben Sie auf der Startseite der Admin-Oberfl√§che einen g√ºltigen API-Key in das Textfeld "API-Key eingeben" ein. Dies authentifiziert Ihre Sitzung f√ºr den Zugriff auf die API-Funktionen.

3.  **Geplante Tasks verwalten:**  Im Bereich "Geplante Tasks" k√∂nnen Sie:
    *   **Tasks anzeigen:**  Eine Liste aller geplanten Tasks mit Details wie URL, Zeitplan, Status etc. wird angezeigt.
    *   **Task-Details erweitern:**  Klicken Sie auf den Expander-Button neben der Task-ID, um detaillierte Informationen zu einem Task anzuzeigen (URL, Zeitplan, Parameter, Status, letzte Ausf√ºhrung, n√§chste Ausf√ºhrung, Fehlermeldungen).
    *   **Tasks l√∂schen:**  Klicken Sie innerhalb der erweiterten Task-Details auf den "Task [Task-ID] l√∂schen" Button, um einen Task dauerhaft zu entfernen.
    *   **Tasks sofort ausf√ºhren:** Klicken Sie innerhalb der erweiterten Task-Details auf den "Task [Task-ID] sofort ausf√ºhren" Button, um einen Task manuell und unabh√§ngig vom Zeitplan zu starten.

4.  **Neue Tasks hinzuf√ºgen:**  Im Bereich "Neuen Task hinzuf√ºgen" k√∂nnen Sie √ºber ein Formular neue geplante Tasks erstellen:
    *   **URL:** Geben Sie die Start-URL der Webseite ein, die gescraped werden soll.
    *   **Zeitplan:** Definieren Sie den Zeitplan f√ºr die Task-Ausf√ºhrung. Unterst√ºtzte Formate sind: `st√ºndlich`, `t√§glich um HH:MM`, `alle X minuten`.
    *   **Nur Text extrahieren:**  Aktivieren Sie diese Checkbox, um nur den Textinhalt anstelle des gesamten HTML-Codes zu extrahieren und zu speichern.
    *   **Stopw√∂rter:**  Optional: Geben Sie eine kommagetrennte Liste von zus√§tzlichen Stopw√∂rtern f√ºr die Keyword-Extraktion an.
    *   **CSS-Selektoren (JSON):** Optional: Geben Sie einen JSON-String mit CSS-Selektoren an, um strukturierte Daten von den Webseiten zu extrahieren.
    *   **Datei speichern:**  Aktivieren Sie diese Checkbox, um den extrahierten Inhalt zus√§tzlich zur Datenbank in einer Datei zu speichern.
    *   **Verarbeitungsfunktion (Pfad):** Optional: Geben Sie den Pfad zu einer Python-Datei an, die eine benutzerdefinierte Verarbeitungsfunktion (`process_data`) enth√§lt.
    *   **Task hinzuf√ºgen Button:**  Klicken Sie auf diesen Button, um den neuen Task zu erstellen und in der Datenbank zu speichern.

5.  **Datenbank Browser Oberfl√§che üñ•Ô∏è‚å®Ô∏è ‚Äì Datenbankinhalte durchsuchen:**

    1.  **Datenbankbrowser starten:** Starten Sie die Streamlit Datenbankbrowser-Oberfl√§che √ºber die Kommandozeile:

        ```bash
        streamlit run db_browser.py
        ```

        *   Die Datenbankbrowser-Oberfl√§che ist nun unter der Adresse `http://localhost:8501` erreichbar (kann je nach Streamlit Konfiguration variieren).

    2.  **API-Key eingeben:** Geben Sie auf der Startseite den ben√∂tigten API-Key ein, um sich zu authentifizieren.

    3.  **Suchparameter festlegen:**
        *   **Suchbegriff:** Geben Sie im Textfeld "Suchbegriff" den Suchbegriff ein, nach dem Sie in der Datenbank suchen m√∂chten (z.B. ein Wort, eine URL, ein Teil eines Titels).
        *   **Suchfeld:** W√§hlen Sie im Dropdown-Men√º "Suchfeld" das Feld aus, in dem gesucht werden soll. Verf√ºgbare Optionen sind: `url`, `title`, `meta_description`, `text_content`, `domain`.

    4.  **Suche starten:** Klicken Sie auf den Button "Suchen", um die Datenbankabfrage mit den angegebenen Parametern zu starten.

    5.  **Suchergebnisse anzeigen:**
        *   **DataFrame-Anzeige:** Die Suchergebnisse werden als interaktiver Pandas DataFrame unterhalb des Suchformulars angezeigt. Die Tabelle enth√§lt Spalten f√ºr `url`, `title`, `meta_description`, `domain` und (gek√ºrzt) `text_content`.
        *   **Keine Ergebnisse:** Wenn keine Eintr√§ge gefunden werden, die dem Suchbegriff entsprechen, wird eine entsprechende Meldung "Keine Ergebnisse gefunden." angezeigt.
        *   **Fehlermeldungen:** Bei Fehlern w√§hrend der Datenbankabfrage oder API-Kommunikation werden Fehlermeldungen oberhalb der Suchergebnisse angezeigt, um den Benutzer √ºber Probleme zu informieren.

**Bedienungshinweise f√ºr die Admin- und Datenbankbrowser-Oberfl√§che:**

*   **API-Key erforderlich:**  F√ºr den Zugriff auf die Funktionen der Admin- und Datenbankbrowser-Oberfl√§che ist die Eingabe eines g√ºltigen API-Keys erforderlich. Stellen Sie sicher, dass die API-Keys korrekt in der `.env` Datei oder `config.yaml` konfiguriert sind und der API-Server l√§uft.
*   **Echtzeit-Aktualisierung:**  √Ñnderungen an geplanten Tasks (Hinzuf√ºgen, Aktualisieren, L√∂schen) in der Admin-Oberfl√§che werden in Echtzeit in der Datenbank gespeichert und vom Scheduler ber√ºcksichtigt.
*   **Browser-Neuladen:**  In einigen F√§llen kann es notwendig sein, die Seite im Browser neu zu laden, um sicherzustellen, dass die aktuellsten Daten und Task-Listen angezeigt werden.
*   **Zeitpl√§ne und CSS-Selektoren:**  Achten Sie darauf, Zeitpl√§ne im korrekten Format einzugeben und CSS-Selektoren als validen JSON-String zu formatieren, um Validierungsfehler zu vermeiden.
*   **Lange `text_content` Spalten:**  Im Datenbankbrowser wird die Spalte `text_content` aus Performance- und Darstellungsgr√ºnden auf die ersten 200 Zeichen gek√ºrzt. Um den vollst√§ndigen Textinhalt anzuzeigen, verwenden Sie ein externes Datenbank-Tool oder passen Sie den Code der `db_browser.py` App an.

## 5. Funktionsbeschreibung

### 5.1 Geplante Tasks erstellen (API und Admin-Oberfl√§che) ‚ûïüìù

**Request Body (JSON) f√ºr API Task-Erstellung:**

```json
{
  "url": "https://www.example.com",
  "schedule_time": "t√§glich um 08:00",
  "text_only": false,
  "stopwords": "example,test",
  "css_selectors": "{\"title\": \"h1\"}",
  "save_file": true,
  "processing_function_path": "custom_processing.py"
}
```

**Parameter:** `url`, `schedule_time`, `text_only`, `stopwords`, `css_selectors`, `save_file`, `processing_function_path`.

### 5.2 Geplante Tasks aktualisieren (API und Admin-Oberfl√§che) üîÑüìù

**Request Body (JSON) f√ºr API Task-Aktualisierung:**

```json
{
  "schedule_time": "st√ºndlich",
  "stopwords": "neue,stopw√∂rter"
}
```

**Parameter:** `task_id` (Pfadparameter), Request Body (JSON) mit zu aktualisierenden Feldern.

### 5.3 Geplante Tasks l√∂schen (API und Admin-Oberfl√§che) ‚ùåüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.4 Geplante Tasks manuell ausf√ºhren (API und Admin-Oberfl√§che) ‚ñ∂Ô∏èüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.5 Webseiteninhalt abrufen und extrahieren (API und Kommandozeile) üåê‚û°Ô∏èüìÑ

**Prozessablauf:**

1.  URL-Validierung ‚úÖ
2.  Cache-Pr√ºfung üóÑÔ∏è
3.  Webseitenabruf (aiohttp prim√§r, Selenium Fallback bei Bedarf) üåê
4.  HTML-Parsing (Beautiful Soup) ü•£
5.  Datenextraktion (Text, Titel, Meta-Description, H1-Headings, Keywords, CSS-Daten) üìÑ
6.  Benutzerdefinierte Datenverarbeitung (optional) ‚öôÔ∏è
7.  Datenbank-Speicherung (SQLite) üíæ
8.  Datei-Speicherung (optional) üóÇÔ∏è
9.  Antwortgenerierung (API) / Ausgabe (Kommandozeile) üì§

### 5.6 Keyword-Extraktion üîëüßÆ

*   Textvorverarbeitung, Stopwortfilterung, alphabetische Filterung, Worth√§ufigkeitsz√§hlung, Top-N Keywords.

### 5.7 CSS-Datenextraktion üß±

*   Einfache und konfigurierte Selektoren (mit `selector`, `type`, `cleanup`).
*   Sicherheitspr√ºfung f√ºr CSS-Selektoren.

### 5.8 Benutzerdefinierte Datenverarbeitung ‚öôÔ∏è

*   `process_data(data)` Funktion in Python-Datei definieren.
*   Pfad zur Datei in Programm/Task konfigurieren.
*   Sicherheitswarnung beachten.‚ö†Ô∏è

## 6. Beispielhafte Anwendungsf√§lle

*   Einmaliges Scrapen √ºber Kommandozeile üöÄ
*   Regelm√§√üiges Scrapen mit geplantem Task ‚è±Ô∏è
*   Extrahieren von Produktinformationen mit CSS-Selektoren üõçÔ∏è
*   Datenanalyse mit benutzerdefinierter Processing-Funktion üìä
*   Abrufen von Links √ºber API üîó
*   Datenbankinhalte mit Streamlit Datenbankbrowser durchsuchen ‚å®Ô∏èüñ•Ô∏è

## 7. Fehlerbehebung

**H√§ufige Fehlermeldungen und L√∂sungen:**

*   "Ung√ºltige URL" ‚ùåüåê - √úberpr√ºfen Sie die eingegebene URL auf Korrektheit und Format. Stellen Sie sicher, dass die URL mit `http://` oder `https://` beginnt.
*   "Webseiteninhalt konnte nicht abgerufen werden" ‚ùå - M√∂gliche Ursachen: Webseite nicht erreichbar, Serverprobleme, Netzwerkprobleme, blockiert durch Firewall/Robot.txt. √úberpr√ºfen Sie die Webseite manuell im Browser. Erh√∂hen Sie ggf. `max_retries` und `retry_delay` in `config.yaml`.
*   "API-Key fehlt oder ist ung√ºltig." ‚ùåüîë - Stellen Sie sicher, dass Sie einen g√ºltigen API-Key im `X-API-Key` Header (API-Anfragen) oder im Streamlit UI eingegeben haben. √úberpr√ºfen Sie die API-Key Konfiguration in `.env` und `config.yaml`. Generieren Sie ggf. neue Keys mit `key_generator.py`.
*   "Rate Limit √ºberschritten. Bitte warten Sie eine Minute." ‚è≥ - Die API ist ratenlimitiert. Reduzieren Sie die Anfragerate oder erh√∂hen Sie `rate_limit_requests_per_minute` in `config.yaml` (mit Vorsicht!).
*   "Ung√ºltiges JSON-Format f√ºr CSS-Selektoren." ‚ùåüß± - √úberpr√ºfen Sie den JSON-String f√ºr CSS-Selektoren auf korrekte Syntax. Verwenden Sie einen JSON-Validator, um Fehler zu finden.
*   "Ung√ºltiger Pfad zur Processing-Funktion" ‚ùå‚öôÔ∏è - Stellen Sie sicher, dass der angegebene Pfad zur Python-Datei der Processing-Funktion korrekt ist und die Datei existiert. Stellen Sie sicher, dass der Pfad relativ zum `processing_functions_dir` in `config.yaml` korrekt ist oder ein absoluter Pfad verwendet wird.
*   "Fehler beim Speichern in die Datenbank" ‚ùåüíæ - M√∂gliche Datenbankfehler. √úberpr√ºfen Sie die Datenbankdatei (`webdata.db`) auf Integrit√§t und Berechtigungen. Pr√ºfen Sie die Server-Logs auf detailliertere Datenbankfehlermeldungen.
*   "Fehler in der Datenverarbeitungsfunktion" ‚ùå‚öôÔ∏è - √úberpr√ºfen Sie die Log-Ausgabe auf Fehlermeldungen aus Ihrer benutzerdefinierten Processing-Funktion. Debuggen Sie die Funktion auf Fehler.
*   "Kritischer Datenbankfehler im Scheduled Mode. Programm wird beendet." ‚ò†Ô∏èüíæ - Ein schwerwiegender Datenbankfehler ist aufgetreten, der den Scheduled Mode beeintr√§chtigt. √úberpr√ºfen Sie die Datenbankintegrit√§t und -konfiguration. Starten Sie das Programm neu. Pr√ºfen Sie die Logs auf detaillierte Fehlermeldungen.

**Log-Level Konfiguration:**

Konfigurierbar in `config.yaml` unter `log_level`.

**Verf√ºgbare Log-Level:**

*   `DEBUG` (detaillierteste Protokollierung) üêõ
*   `INFO` (Standard) ‚ÑπÔ∏è
*   `WARNING` ‚ö†Ô∏è
*   `ERROR` ‚ùå
*   `CRITICAL` ‚ò†Ô∏è

**Beispiel `config.yaml` f√ºr `DEBUG` Log-Level:**

```yaml
log_level: DEBUG
```

## 8. FAQ (H√§ufig gestellte Fragen)

**F: Wie konfiguriere ich API-Keys?**

A: API-Keys k√∂nnen in der `config.yaml` Datei unter `api_keys` als Liste von Strings oder sicherer √ºber Umgebungsvariablen (siehe Abschnitt 3.1) konfiguriert werden. Der empfohlene Weg ist die Verwendung des `key_generator.py` Skripts (siehe Installationsschritt 7).

**F: Wie √§ndere ich das Rate Limit der API?**

A: Das Rate Limit (maximale Anfragen pro Minute) kann in der `config.yaml` Datei unter `rate_limit_requests_per_minute` konfiguriert werden.

**F: Wie lange werden Webseiten im Cache gespeichert?**

A: Die G√ºltigkeitsdauer des Caches (in Sekunden) kann in der `config.yaml` Datei unter `cache_expiry_seconds` konfiguriert werden. Standardm√§√üig sind es 600 Sekunden (10 Minuten).

**F: Wie kann ich geplante Tasks verwalten?**

A: Geplante Tasks k√∂nnen √ºber die Streamlit Admin-Oberfl√§che (empfohlen) oder √ºber die Web-API verwaltet werden (erstellen, aktualisieren, l√∂schen, auflisten, manuell ausf√ºhren, Status abrufen).

**F: Wo werden die gescrapten Daten gespeichert?**

A: Die gescrapten Daten werden in einer SQLite-Datenbank gespeichert. Der Pfad zur Datenbankdatei kann in der `config.yaml` Datei unter `database_file` konfiguriert werden. Standardm√§√üig ist dies `webdata.db` im Projektverzeichnis.

**F: Kann ich nur Textinhalte extrahieren?**

A: Ja, Sie k√∂nnen nur Textinhalte extrahieren, indem Sie die Option `--text` in der Kommandozeile verwenden, `text_only=true` im Request Body f√ºr die API-Endpunkte `/fetch-html` und `/fetch-text` setzen oder die Checkbox "Nur Text extrahieren" in der Streamlit Admin-Oberfl√§che aktivieren.

**F: Wie kann ich benutzerdefinierte Stopw√∂rter verwenden?**

A: Benutzerdefinierte Stopw√∂rter k√∂nnen als kommagetrennte Liste √ºber die Option `--stopwords` in der Kommandozeile, den Parameter `stopwords` in den API-Endpunkten `/fetch-html` und `/fetch-text` oder das Textfeld "Stopw√∂rter" in der Streamlit Admin-Oberfl√§che angegeben werden.

**F: Sind CSS-Selektoren sicher zu verwenden?**

A: WebCrawler-Pro implementiert Sicherheitspr√ºfungen f√ºr CSS-Selektoren, um potenziell unsichere Selektoren zu erkennen und zu verhindern. Dennoch sollten Sie bei der Verwendung von CSS-Selektoren Vorsicht walten lassen und nur vertrauensw√ºrdige Selektoren verwenden.

**F: Sind benutzerdefinierte Processing-Funktionen sicher?**

A: Benutzerdefinierte Processing-Funktionen k√∂nnen beliebigen Python-Code ausf√ºhren. Verwenden Sie diese Funktion nur mit Bedacht und stellen Sie sicher, dass Sie nur vertrauensw√ºrdigen Code ausf√ºhren, um Sicherheitsrisiken zu vermeiden. WebCrawler-Pro validiert den Pfad zur Processing-Funktion, um unsichere Pfade zu verhindern.

**F: Unterst√ºtzt WebCrawler-Pro JavaScript-Rendering?**

A: Ja, WebCrawler-Pro verwendet `aiohttp` f√ºr schnelle Abrufe und Selenium und ChromeDriver als Fallback, um auch Webseiten mit dynamischen JavaScript-Inhalten abzurufen und zu verarbeiten.

## 9. Glossar

*   **API (Application Programming Interface):** üåê Eine Schnittstelle, die es Softwareanwendungen erm√∂glicht, miteinander zu kommunizieren. Im Kontext von WebCrawler-Pro erm√∂glicht die API den programmatischen Zugriff auf Web-Scraping-Funktionalit√§ten.
*   **CSS-Selektor:** üß± Ein Muster, das verwendet wird, um HTML-Elemente auf einer Webseite auszuw√§hlen und zu formatieren oder Daten aus diesen Elementen zu extrahieren.
*   **ChromeDriver:** üåê Ein separates ausf√ºhrbares Programm, das von Selenium verwendet wird, um Chrome-Browser zu steuern.
*   **JSON (JavaScript Object Notation):** üìÑ Ein leichtgewichtiges Datenformat, das f√ºr den Datenaustausch im Web verwendet wird.
*   **Rate Limiting:** ‚è≥ Eine Technik zur Begrenzung der Anzahl von Anfragen, die ein Benutzer oder eine Anwendung innerhalb eines bestimmten Zeitraums an eine API senden kann. Dies dient dem Schutz vor √úberlastung und Missbrauch.
*   **Caching:** üóÑÔ∏è Eine Technik zur Speicherung h√§ufig abgerufener Daten (z.B. Webseiteninhalte) in einem tempor√§ren Speicher (Cache), um den Zugriff zu beschleunigen und die Last auf den urspr√ºnglichen Datenquelle zu reduzieren.
*   **Scheduled Task (Geplanter Task):** ‚è±Ô∏èüìù Eine Aufgabe, die automatisch zu einem vordefinierten Zeitpunkt oder in regelm√§√üigen Intervallen ausgef√ºhrt wird. Im Kontext von WebCrawler-Pro sind geplante Tasks Web-Scraping-Aufgaben, die automatisch nach Zeitplan ausgef√ºhrt werden.
*   **Selenium:** üåê Ein Framework f√ºr die Automatisierung von Webbrowsern. WebCrawler-Pro verwendet Selenium als Fallback, um Webseiten dynamisch abzurufen und JavaScript-Inhalte zu rendern, falls der prim√§re Abruf mit `aiohttp` fehlschl√§gt.
*   **aiohttp:** üöÄ Eine Python-Bibliothek f√ºr asynchrone HTTP-Client-/Server-Kommunikation. WebCrawler-Pro verwendet `aiohttp` als prim√§re Methode f√ºr schnelle und effiziente Webseitenabrufe.

## 10. Kontakt und Support

**E-Mail:** üìß support@ciphercore.de

**Webseite:** üåê www.ciphercore.de 

Bitte beschreiben Sie Ihr Problem/Anfrage detailliert.
