# üöÄ WebCrawler-Pro ‚Äì Dokumentation und Bedienungsanleitung

## 1. Einleitung

**WebCrawler-Pro** ist ein vielseitiges und leistungsstarkes Programm zum automatisierten Extrahieren von Webinhalten. Es erm√∂glicht das Abrufen von HTML- und Textinhalten von Webseiten, die Extraktion strukturierter Daten mithilfe von CSS-Selektoren, die Keyword-Analyse und die optionale benutzerdefinierte Verarbeitung der gewonnenen Daten. WebCrawler-Pro bietet eine flexible Nutzung √ºber die Kommandozeile, eine programmatische API und eine intuitive Streamlit-basierte Admin-Oberfl√§che.

**üéØ Zweck:**

*   ‚úÖ Automatisierte Datenerfassung aus dem Web f√ºr Forschungszwecke, Marktanalysen, Content-Aggregation und mehr.
*   ‚è±Ô∏è Regelm√§√üige √úberwachung von Webseiteninhalten durch zeitgesteuerte Tasks.
*   üåê Bereitstellung einer API f√ºr den Zugriff auf Web-Scraping-Funktionalit√§ten in anderen Anwendungen.
*   üñ•Ô∏è Einfache Verwaltung und √úberwachung geplanter Web-Scraping-Aufgaben √ºber eine Web-Oberfl√§che.

**üë• Zielgruppe:**

*   üìä Datenanalysten und Wissenschaftler, die gro√üe Mengen an Webinhalten f√ºr ihre Analysen ben√∂tigen.
*   üßë‚Äçüíª Softwareentwickler, die Web-Scraping-Funktionalit√§ten in ihre Anwendungen integrieren m√∂chten.
*   üì¢ Content-Manager und Marketingexperten, die Webinhalte √ºberwachen und analysieren m√ºssen.
*   üõ†Ô∏è Technische Anwender, die eine flexible und konfigurierbare L√∂sung f√ºr Web-Scraping suchen.

## 2. Systemanforderungen

Um WebCrawler-Pro nutzen zu k√∂nnen, m√ºssen folgende Systemvoraussetzungen erf√ºllt sein:

**üíª Hardware:**

*   **Prozessor:** Intel Core i3 oder vergleichbarer Prozessor (empfohlen: Intel Core i5 oder besser)
*   **Arbeitsspeicher:** Mindestens 4 GB RAM (empfohlen: 8 GB RAM oder mehr, insbesondere f√ºr umfangreiche Scraping-Aufgaben und den API-Betrieb)
*   **Festplattenspeicher:** Mindestens 1 GB freier Festplattenspeicher f√ºr die Programminstallation und die Datenbank (der ben√∂tigte Speicherplatz kann je nach Umfang der gescrapten Daten variieren)

**üíæ Software:**

*   **Betriebssystem:** Windows 10 oder h√∂her, macOS 10.15 oder h√∂her, Linux (getestet auf Ubuntu 20.04 und neuer)
*   **Python:** Python 3.8 oder h√∂her (empfohlen: Python 3.9 oder 3.10). Stellen Sie sicher, dass Python und `pip` im Systempfad verf√ºgbar sind.
*   **Webbrowser:** Google Chrome (f√ºr den Betrieb mit Selenium). Es wird empfohlen, die aktuellste stabile Version von Chrome zu verwenden.
*   **ChromeDriver:** Der zu Ihrer Chrome-Version passende ChromeDriver wird automatisch durch `webdriver-manager` installiert.

**üêç Python Bibliotheken:**

Die folgenden Python-Bibliotheken sind f√ºr den Betrieb von WebCrawler-Pro erforderlich und werden in der Regel bei der Installation automatisch installiert:

*   `streamlit`
*   `pandas`
*   `beautifulsoup4`
*   `selenium`
*   `webdriver_manager`
*   `flask`
*   `sqlite3`
*   `nltk`
*   `pyyaml`
*   `pydantic`
*   `python-dotenv`

## 3. Installation

Folgen Sie diesen Schritten, um WebCrawler-Pro zu installieren:

**Schritt 1: Python installieren**

Laden Sie die aktuelle Version von Python 3.x von der offiziellen Python-Webseite ([https://www.python.org/downloads/](https://www.python.org/downloads/)) herunter und installieren Sie diese. Achten Sie darauf, bei der Installation die Option "Add Python to PATH" zu aktivieren.

**Schritt 2: Projektdateien herunterladen**

Laden Sie die Projektdateien von WebCrawler-Pro herunter und entpacken Sie das Archiv in einen lokalen Ordner.

**Schritt 3: Virtuelle Umgebung erstellen (empfohlen)**

Navigieren Sie im Terminal/Eingabeaufforderung in den Projektordner und erstellen Sie eine virtuelle Umgebung:

```bash
python -m venv venv
```

Aktivieren Sie die virtuelle Umgebung:

*   **Windows (Eingabeaufforderung):** `venv\Scripts\activate`
*   **Windows (PowerShell):** `venv\Scripts\Activate.ps1`
*   **macOS/Linux:** `source venv/bin/activate`

**Schritt 4: Bibliotheken installieren**

Installieren Sie die ben√∂tigten Python-Bibliotheken:

```bash
pip install streamlit pandas beautifulsoup4 selenium webdriver-manager flask pyyaml pydantic nltk python-dotenv
```

**Schritt 5: Konfiguration anpassen (optional)**

Passen Sie die `config.yaml` Datei bei Bedarf an. Wichtige Optionen:

*   `database_file`
*   `api_keys` (oder Umgebungsvariablen verwenden)
*   `rate_limit_requests_per_minute`
*   `cache_expiry_seconds`
*   `selenium_config`
*   `allowed_css_properties`
*   `processing_functions_dir`
*   `log_level`

**Schritt 6: ChromeDriver Installation**

ChromeDriver wird automatisch von `webdriver-manager` installiert.

**Schritt 7: Installation abschlie√üen**

WebCrawler-Pro ist nun installiert.

### 3.1 API-Keys √ºber Umgebungsvariablen konfigurieren üîë

Erstellen Sie eine `.env` Datei im Projektordner und f√ºgen Sie API-Keys hinzu:

```dotenv
API_KEY_1=IhrErsterAPIKey
API_KEY_2=IhrZweiterAPIKey
API_KEY_3=IhrDritterAPIKey
```

### 3.2 Fehlerbehebung bei der Installation üõ†Ô∏è

Siehe vollst√§ndige Dokumentation f√ºr detaillierte Fehlerbehebungshinweise.

## 4. Benutzerf√ºhrung

WebCrawler-Pro kann √ºber Kommandozeile, Web-API oder Streamlit Admin-Oberfl√§che verwendet werden.

### 4.1 Kommandozeilenmodus ‚å®Ô∏è

Navigieren Sie zum Projektordner und aktivieren Sie die virtuelle Umgebung.

**Grundlegende Nutzung:**

```bash
python app.py <URL>
```

**Optionale Argumente:**

*   `--text`
*   `--save-file`
*   `--stopwords <Stopw√∂rter>`
*   `--css-selectors <JSON-String>`
*   `--processing-function <Pfad>`
*   `--api`
*   `--streamlit`

**Beispiele:**

*   `python app.py --text https://www.example.com`
*   `python app.py --save-file --stopwords "zus√§tzlich,weiteres" https://www.example.com`
*   `python app.py --css-selectors '{"title": "h1", "paragraph": "p"}' https://www.example.com`
*   `python app.py --processing-function custom_processing.py https://www.example.com`
*   `python app.py --api`
*   `python app.py --streamlit`
*   `python app.py` (f√ºr Scheduled Mode)

### 4.2 Web-API üåê

Starten Sie die API:

```bash
python app.py --api
```

API ist unter `http://localhost:5000` erreichbar.

**Authentifizierung:** API-Key im `X-API-Key` Header erforderlich (au√üer `/api/v1/health`).

**Rate Limiting & Caching:** Aktiv. Konfigurierbar in `config.yaml`.

**API Endpunkte:**

*   `/api/v1/` (GET) - API √úbersicht
*   `/api/v1/fetch-html` (GET) - HTML Inhalt abrufen
*   `/api/v1/fetch-text` (GET) - Text Inhalt abrufen
*   `/api/v1/fetch-links` (GET) - Links extrahieren
*   `/api/v1/scheduled-tasks` (GET, POST) - Geplante Tasks verwalten (Liste, Erstellen)
*   `/api/v1/scheduled-tasks/<task_id>` (GET, PUT, DELETE) - Geplante Tasks verwalten (Details, Aktualisieren, L√∂schen)
*   `/api/v1/scheduled-tasks/status` (GET) - Task Status √úbersicht
*   `/api/v1/scheduled-tasks/<task_id>/status` (GET) - Task Detail Status
*   `/api/v1/scheduled-tasks/<task_id>/run` (POST) - Task manuell ausf√ºhren
*   `/api/v1/health` (GET) - Health Check (kein API-Key ben√∂tigt)

**Beispiele f√ºr API-Anfragen:**

**Linux/macOS (curl):**

```bash
curl -X GET \
  'http://localhost:5000/api/v1/fetch-text?url=https://www.example.com&stopwords=example,test&css_selectors={"title": "h1"}' \
  -H 'X-API-Key: IhrAPIKey'
```

**Windows (PowerShell - Invoke-WebRequest):**

```powershell
$API_KEY = "IhrAPIKey" # Ersetzen Sie dies mit Ihrem API-Key
$API_HOST = "http://localhost:5000"

$response = Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-text?url=https://www.example.com&stopwords=example,test&css_selectors={\"title\": \"h1\"}" `
    -Headers @{'X-API-Key' = $API_KEY}

# Antwortinhalt als JSON ausgeben (optional)
$response.Content | ConvertFrom-Json
```

**Weitere Windows/PowerShell Beispiele (Invoke-WebRequest):**

*   **Links von einer Webseite abrufen:**

    ```powershell
    $API_KEY = "IhrAPIKey"
    $API_HOST = "http://localhost:5000"
    $BASE_URL = "https://www.example.com"

    $links_response = Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-links?url=$BASE_URL" `
        -Headers @{'X-API-Key' = $API_KEY} | ConvertFrom-Json

    # Links ausgeben (optional)
    $links_response.data.links
    ```

*   **HTML-Inhalt abrufen und in Datei speichern:**

    ```powershell
    $API_KEY = "IhrAPIKey"
    $API_HOST = "http://localhost:5000"
    $URL_TO_FETCH = "https://www.example.com"

    Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-html?url=$URL_TO_FETCH&save_file=true" `
        -Headers @{'X-API-Key' = $API_KEY}
    ```

### 4.3 Streamlit Admin-Oberfl√§che üñ•Ô∏è

Starten Sie die Admin-Oberfl√§che:

```bash
python app.py --streamlit
```

Zugriff √ºber `http://localhost:8501`.

**Benutzeroberfl√§che:**

1.  **API-Key Eingabe** üîë
2.  **Geplante Tasks √úbersicht** üìù (mit Details und Aktionen)
3.  **Neuen Task hinzuf√ºgen**‚ûï (Formular)

**Bedienungshinweise:**

*   API-Key zu Beginn eingeben.
*   "Geplante Tasks" √úbersicht f√ºr Task-Management.
*   "Neuen Task hinzuf√ºgen" f√ºr neue Tasks.
*   Zeitpl√§ne und CSS-Selektoren korrekt formatieren.
*   Seite wird nach Task-√Ñnderungen neu geladen.

## 5. Funktionsbeschreibung

### 5.1 Geplante Tasks erstellen (API und Admin-Oberfl√§che) ‚ûïüìù

**Request Body (JSON) f√ºr API Task-Erstellung:**

```json
{
  "url": "https://www.example.com",
  "schedule_time": "t√§glich um 08:00",
  "text_only": false,
  "stopwords": "example,test",
  "css_selectors": "{\"title\": \"h1\"}",
  "save_file": true,
  "processing_function_path": "custom_processing.py"
}
```

**Parameter:** `url`, `schedule_time`, `text_only`, `stopwords`, `css_selectors`, `save_file`, `processing_function_path`.

### 5.2 Geplante Tasks aktualisieren (API und Admin-Oberfl√§che) üîÑüìù

**Request Body (JSON) f√ºr API Task-Aktualisierung:**

```json
{
  "schedule_time": "st√ºndlich",
  "stopwords": "neue,stopw√∂rter"
}
```

**Parameter:** `task_id` (Pfadparameter), Request Body (JSON) mit zu aktualisierenden Feldern.

### 5.3 Geplante Tasks l√∂schen (API und Admin-Oberfl√§che) ‚ùåüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.4 Geplante Tasks manuell ausf√ºhren (API und Admin-Oberfl√§che) ‚ñ∂Ô∏èüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.5 Webseiteninhalt abrufen und extrahieren (API und Kommandozeile) üåê‚û°Ô∏èüìÑ

**Prozessablauf:**

1.  URL-Validierung ‚úÖ
2.  Cache-Pr√ºfung üóÑÔ∏è
3.  Webseitenabruf (Selenium) üåê
4.  HTML-Parsing (Beautiful Soup) ü•£
5.  Datenextraktion (Text, Titel, Meta-Description, H1-Headings, Keywords, CSS-Daten) üìÑ
6.  Benutzerdefinierte Datenverarbeitung (optional) ‚öôÔ∏è
7.  Datenbank-Speicherung üíæ
8.  Datei-Speicherung (optional) üóÇÔ∏è
9.  Antwortgenerierung (API) / Ausgabe (Kommandozeile) üì§

### 5.6 Keyword-Extraktion üîëüßÆ

*   Textvorverarbeitung, Stopwortfilterung, alphabetische Filterung, Worth√§ufigkeitsz√§hlung, Top-N Keywords.

### 5.7 CSS-Datenextraktion üß±

*   Einfache und konfigurierte Selektoren (mit `selector`, `type`, `cleanup`).
*   Sicherheitspr√ºfung f√ºr CSS-Selektoren.

### 5.8 Benutzerdefinierte Datenverarbeitung ‚öôÔ∏è

*   `process_data(data)` Funktion in Python-Datei definieren.
*   Pfad zur Datei in Programm/Task konfigurieren.
*   Sicherheitswarnung beachten.‚ö†Ô∏è

## 6. Beispielhafte Anwendungsf√§lle

*   Einmaliges Scrapen √ºber Kommandozeile üöÄ
*   Regelm√§√üiges Scrapen mit geplantem Task ‚è±Ô∏è
*   Extrahieren von Produktinformationen mit CSS-Selektoren üõçÔ∏è
*   Datenanalyse mit benutzerdefinierter Processing-Funktion üìä
*   Abrufen von Links √ºber API üîó

## 7. Fehlerbehebung

**H√§ufige Fehlermeldungen und L√∂sungen:**

*   "Ung√ºltige URL" ‚ùåüåê
*   "Webseiteninhalt konnte nicht abgerufen werden" ‚ùå
*   "API-Key fehlt oder ist ung√ºltig." ‚ùåüîë
*   "Rate Limit √ºberschritten. Bitte warten Sie eine Minute." ‚è≥
*   "Ung√ºltiges JSON-Format f√ºr CSS-Selektoren." ‚ùåüß±
*   "Ung√ºltiger Pfad zur Processing-Funktion" ‚ùå‚öôÔ∏è
*   "Fehler beim Speichern in die Datenbank" ‚ùåüíæ
*   "Fehler in der Datenverarbeitungsfunktion" ‚ùå‚öôÔ∏è
*    "Kritischer Datenbankfehler im Scheduled Mode. Programm wird beendet." ‚ò†Ô∏èüíæ

**Log-Level Konfiguration:**

Konfigurierbar in `config.yaml` unter `log_level`.

**Verf√ºgbare Log-Level:**

*   `DEBUG` (detaillierteste Protokollierung) üêõ
*   `INFO` (Standard) ‚ÑπÔ∏è
*   `WARNING` ‚ö†Ô∏è
*   `ERROR` ‚ùå
*   `CRITICAL` ‚ò†Ô∏è

**Beispiel `config.yaml` f√ºr `DEBUG` Log-Level:**

```yaml
log_level: DEBUG
```

## 8. FAQ (H√§ufig gestellte Fragen)

**F: Wie konfiguriere ich API-Keys?**

A: API-Keys k√∂nnen in der `config.yaml` Datei unter `api_keys` als Liste von Strings oder sicherer √ºber Umgebungsvariablen (siehe Abschnitt 3.1) konfiguriert werden.

**F: Wie √§ndere ich das Rate Limit der API?**

A: Das Rate Limit (maximale Anfragen pro Minute) kann in der `config.yaml` Datei unter `rate_limit_requests_per_minute` konfiguriert werden.

**F: Wie lange werden Webseiten im Cache gespeichert?**

A: Die G√ºltigkeitsdauer des Caches (in Sekunden) kann in der `config.yaml` Datei unter `cache_expiry_seconds` konfiguriert werden. Standardm√§√üig sind es 600 Sekunden (10 Minuten).

**F: Wie kann ich geplante Tasks verwalten?**

A: Geplante Tasks k√∂nnen √ºber die Streamlit Admin-Oberfl√§che (empfohlen) oder √ºber die Web-API verwaltet werden (erstellen, aktualisieren, l√∂schen, auflisten, manuell ausf√ºhren, Status abrufen).

**F: Wo werden die gescrapten Daten gespeichert?**

A: Die gescrapten Daten werden in einer SQLite-Datenbank gespeichert. Der Pfad zur Datenbankdatei kann in der `config.yaml` Datei unter `database_file` konfiguriert werden.

**F: Kann ich nur Textinhalte extrahieren?**

A: Ja, Sie k√∂nnen nur Textinhalte extrahieren, indem Sie die Option `--text` in der Kommandozeile verwenden, `text_only=true` im Request Body f√ºr die API-Endpunkte `/fetch-html` und `/fetch-text` setzen oder die Checkbox "Nur Text extrahieren" in der Streamlit Admin-Oberfl√§che aktivieren.

**F: Wie kann ich benutzerdefinierte Stopw√∂rter verwenden?**

A: Benutzerdefinierte Stopw√∂rter k√∂nnen als kommagetrennte Liste √ºber die Option `--stopwords` in der Kommandozeile, den Parameter `stopwords` in den API-Endpunkten `/fetch-html` und `/fetch-text` oder das Textfeld "Stopw√∂rter" in der Streamlit Admin-Oberfl√§che angegeben werden.

**F: Sind CSS-Selektoren sicher zu verwenden?**

A: WebCrawler-Pro implementiert Sicherheitspr√ºfungen f√ºr CSS-Selektoren, um potenziell unsichere Selektoren zu erkennen und zu verhindern. Dennoch sollten Sie bei der Verwendung von CSS-Selektoren Vorsicht walten lassen und nur vertrauensw√ºrdige Selektoren verwenden.

**F: Sind benutzerdefinierte Processing-Funktionen sicher?**

A: Benutzerdefinierte Processing-Funktionen k√∂nnen beliebigen Python-Code ausf√ºhren. Verwenden Sie diese Funktion nur mit Bedacht und stellen Sie sicher, dass Sie nur vertrauensw√ºrdigen Code ausf√ºhren, um Sicherheitsrisiken zu vermeiden. WebCrawler-Pro validiert den Pfad zur Processing-Funktion, um unsichere Pfade zu verhindern.

**F: Unterst√ºtzt WebCrawler-Pro JavaScript-Rendering?**

A: Ja, WebCrawler-Pro verwendet Selenium und ChromeDriver, um Webseiten abzurufen, was das Rendering von JavaScript-Inhalten erm√∂glicht.

## 9. Glossar

*   **API (Application Programming Interface):** üåê Eine Schnittstelle, die es Softwareanwendungen erm√∂glicht, miteinander zu kommunizieren. Im Kontext von WebCrawler-Pro erm√∂glicht die API den programmatischen Zugriff auf Web-Scraping-Funktionalit√§ten.
*   **CSS-Selektor:** üß± Ein Muster, das verwendet wird, um HTML-Elemente auf einer Webseite auszuw√§hlen und zu formatieren oder Daten aus diesen Elementen zu extrahieren.
*   **ChromeDriver:** üåê Ein separates ausf√ºhrbares Programm, das von Selenium verwendet wird, um Chrome-Browser zu steuern.
*   **JSON (JavaScript Object Notation):** üìÑ Ein leichtgewichtiges Datenformat, das f√ºr den Datenaustausch im Web verwendet wird.
*   **Rate Limiting:** ‚è≥ Eine Technik zur Begrenzung der Anzahl von Anfragen, die ein Benutzer oder eine Anwendung innerhalb eines bestimmten Zeitraums an eine API senden kann. Dies dient dem Schutz vor √úberlastung und Missbrauch.
*   **Caching:** üóÑÔ∏è Eine Technik zur Speicherung h√§ufig abgerufener Daten (z.B. Webseiteninhalte) in einem tempor√§ren Speicher (Cache), um den Zugriff zu beschleunigen und die Last auf den urspr√ºnglichen Datenquelle zu reduzieren.
*   **Scheduled Task (Geplanter Task):** ‚è±Ô∏èüìù Eine Aufgabe, die automatisch zu einem vordefinierten Zeitpunkt oder in regelm√§√üigen Intervallen ausgef√ºhrt wird. Im Kontext von WebCrawler-Pro sind geplante Tasks Web-Scraping-Aufgaben, die automatisch nach Zeitplan ausgef√ºhrt werden.
*   **Selenium:** üåê Ein Framework f√ºr die Automatisierung von Webbrowsern. WebCrawler-Pro verwendet Selenium, um Webseiten dynamisch abzurufen und JavaScript-Inhalte zu rendern.

## 10. Kontakt und Support

**E-Mail:** üìß support@webcrawler-pro.example.com (Platzhalter E-Mail Adresse)

**Webseite:** üåê www.webcrawler-pro.example.com (Platzhalter Webseite)

Bitte beschreiben Sie Ihr Problem/Anfrage detailliert.


