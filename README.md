# ğŸš€ WebCrawler-Pro â€“ Dokumentation und Bedienungsanleitung

## 1. Einleitung

**WebCrawler-Pro** ist ein vielseitiges und leistungsstarkes Programm zum automatisierten Extrahieren von Webinhalten. Es ermÃ¶glicht das Abrufen von HTML- und Textinhalten von Webseiten, die Extraktion strukturierter Daten mithilfe von CSS-Selektoren, die Keyword-Analyse und die optionale benutzerdefinierte Verarbeitung der gewonnenen Daten. WebCrawler-Pro bietet eine flexible Nutzung Ã¼ber die Kommandozeile, eine programmatische API und eine intuitive Streamlit-basierte Admin-OberflÃ¤che.

**ğŸ¯ Zweck:**

*   âœ… Automatisierte Datenerfassung aus dem Web fÃ¼r Forschungszwecke, Marktanalysen, Content-Aggregation und mehr.
*   â±ï¸ RegelmÃ¤ÃŸige Ãœberwachung von Webseiteninhalten durch zeitgesteuerte Tasks.
*   ğŸŒ Bereitstellung einer API fÃ¼r den Zugriff auf Web-Scraping-FunktionalitÃ¤ten in anderen Anwendungen.
*   ğŸ–¥ï¸ Einfache Verwaltung und Ãœberwachung geplanter Web-Scraping-Aufgaben Ã¼ber eine Web-OberflÃ¤che.

**ğŸ‘¥ Zielgruppe:**

*   ğŸ“Š Datenanalysten und Wissenschaftler, die groÃŸe Mengen an Webinhalten fÃ¼r ihre Analysen benÃ¶tigen.
*   ğŸ§‘â€ğŸ’» Softwareentwickler, die Web-Scraping-FunktionalitÃ¤ten in ihre Anwendungen integrieren mÃ¶chten.
*   ğŸ“¢ Content-Manager und Marketingexperten, die Webinhalte Ã¼berwachen und analysieren mÃ¼ssen.
*   ğŸ› ï¸ Technische Anwender, die eine flexible und konfigurierbare LÃ¶sung fÃ¼r Web-Scraping suchen.

## 2. Systemanforderungen

Um WebCrawler-Pro nutzen zu kÃ¶nnen, mÃ¼ssen folgende Systemvoraussetzungen erfÃ¼llt sein:

**ğŸ’» Hardware:**

*   **Prozessor:** Intel Core i3 oder vergleichbarer Prozessor (empfohlen: Intel Core i5 oder besser)
*   **Arbeitsspeicher:** Mindestens 4 GB RAM (empfohlen: 8 GB RAM oder mehr, insbesondere fÃ¼r umfangreiche Scraping-Aufgaben und den API-Betrieb)
*   **Festplattenspeicher:** Mindestens 1 GB freier Festplattenspeicher fÃ¼r die Programminstallation und die Datenbank (der benÃ¶tigte Speicherplatz kann je nach Umfang der gescrapten Daten variieren)

**ğŸ’¾ Software:**

*   **Betriebssystem:** Windows 10 oder hÃ¶her, macOS 10.15 oder hÃ¶her, Linux (getestet auf Ubuntu 20.04 und neuer)
*   **Python:** Python 3.8 oder hÃ¶her (empfohlen: Python 3.9 oder 3.10). Stellen Sie sicher, dass Python und `pip` im Systempfad verfÃ¼gbar sind.
*   **Webbrowser:** Google Chrome (fÃ¼r den Betrieb mit Selenium). Es wird empfohlen, die aktuellste stabile Version von Chrome zu verwenden.
*   **ChromeDriver:** Der zu Ihrer Chrome-Version passende ChromeDriver wird automatisch durch `webdriver-manager` installiert.

**ğŸ Python Bibliotheken:**

Die folgenden Python-Bibliotheken sind fÃ¼r den Betrieb von WebCrawler-Pro erforderlich und werden in der Regel bei der Installation automatisch installiert:

*   `streamlit`
*   `pandas`
*   `beautifulsoup4`
*   `selenium`
*   `webdriver_manager`
*   `flask`
*   `sqlite3`
*   `nltk`
*   `pyyaml`
*   `pydantic`
*   `python-dotenv`

## 3. Installation

Folgen Sie diesen Schritten, um WebCrawler-Pro zu installieren:

**Schritt 1: Python installieren**

Laden Sie die aktuelle Version von Python 3.x von der offiziellen Python-Webseite ([https://www.python.org/downloads/](https://www.python.org/downloads/)) herunter und installieren Sie diese. Achten Sie darauf, bei der Installation die Option "Add Python to PATH" zu aktivieren.

**Schritt 2: Projektdateien herunterladen**

Laden Sie die Projektdateien von WebCrawler-Pro herunter und entpacken Sie das Archiv in einen lokalen Ordner.

**Schritt 3: Virtuelle Umgebung erstellen (empfohlen)**

Navigieren Sie im Terminal/Eingabeaufforderung in den Projektordner und erstellen Sie eine virtuelle Umgebung:

```bash
python -m venv venv
```

Aktivieren Sie die virtuelle Umgebung:

*   **Windows (Eingabeaufforderung):** `venv\Scripts\activate`
*   **Windows (PowerShell):** `venv\Scripts\Activate.ps1`
*   **macOS/Linux:** `source venv/bin/activate`

**Schritt 4: Bibliotheken installieren**

Installieren Sie die benÃ¶tigten Python-Bibliotheken:

```bash
pip install streamlit pandas beautifulsoup4 selenium webdriver-manager flask pyyaml pydantic nltk python-dotenv
```

**Schritt 5: Konfiguration anpassen (optional)**

Passen Sie die `config.yaml` Datei bei Bedarf an. Wichtige Optionen:

*   `database_file`
*   `api_keys` (oder Umgebungsvariablen verwenden)
*   `rate_limit_requests_per_minute`
*   `cache_expiry_seconds`
*   `selenium_config`
*   `allowed_css_properties`
*   `processing_functions_dir`
*   `log_level`

**Schritt 6: ChromeDriver Installation**

ChromeDriver wird automatisch von `webdriver-manager` installiert.

**Schritt 7: Installation abschlieÃŸen**

WebCrawler-Pro ist nun installiert.

### 3.1 API-Keys Ã¼ber Umgebungsvariablen konfigurieren ğŸ”‘

Erstellen Sie eine `.env` Datei im Projektordner und fÃ¼gen Sie API-Keys hinzu:

```dotenv
API_KEY_1=IhrErsterAPIKey
API_KEY_2=IhrZweiterAPIKey
API_KEY_3=IhrDritterAPIKey
```

### 3.2 Fehlerbehebung bei der Installation ğŸ› ï¸

Siehe vollstÃ¤ndige Dokumentation fÃ¼r detaillierte Fehlerbehebungshinweise.

## 4. BenutzerfÃ¼hrung

WebCrawler-Pro kann Ã¼ber Kommandozeile, Web-API oder Streamlit Admin-OberflÃ¤che verwendet werden.

### 4.1 Kommandozeilenmodus âŒ¨ï¸

Navigieren Sie zum Projektordner und aktivieren Sie die virtuelle Umgebung.

**Grundlegende Nutzung:**

```bash
python app.py <URL>
```

**Optionale Argumente:**

*   `--text`
*   `--save-file`
*   `--stopwords <StopwÃ¶rter>`
*   `--css-selectors <JSON-String>`
*   `--processing-function <Pfad>`
*   `--api`
*   `--streamlit`

**Beispiele:**

*   `python app.py --text https://www.example.com`
*   `python app.py --save-file --stopwords "zusÃ¤tzlich,weiteres" https://www.example.com`
*   `python app.py --css-selectors '{"title": "h1", "paragraph": "p"}' https://www.example.com`
*   `python app.py --processing-function custom_processing.py https://www.example.com`
*   `python app.py --api`
*   `python app.py --streamlit`
*   `python app.py` (fÃ¼r Scheduled Mode)

### 4.2 Web-API ğŸŒ

Starten Sie die API:

```bash
python app.py --api
```

API ist unter `http://localhost:5000` erreichbar.

**Authentifizierung:** API-Key im `X-API-Key` Header erforderlich (auÃŸer `/api/v1/health`).

**Rate Limiting & Caching:** Aktiv. Konfigurierbar in `config.yaml`.

**API Endpunkte:**

*   `/api/v1/` (GET) - API Ãœbersicht
*   `/api/v1/fetch-html` (GET) - HTML Inhalt abrufen
*   `/api/v1/fetch-text` (GET) - Text Inhalt abrufen
*   `/api/v1/fetch-links` (GET) - Links extrahieren
*   `/api/v1/scheduled-tasks` (GET, POST) - Geplante Tasks verwalten (Liste, Erstellen)
*   `/api/v1/scheduled-tasks/<task_id>` (GET, PUT, DELETE) - Geplante Tasks verwalten (Details, Aktualisieren, LÃ¶schen)
*   `/api/v1/scheduled-tasks/status` (GET) - Task Status Ãœbersicht
*   `/api/v1/scheduled-tasks/<task_id>/status` (GET) - Task Detail Status
*   `/api/v1/scheduled-tasks/<task_id>/run` (POST) - Task manuell ausfÃ¼hren
*   `/api/v1/health` (GET) - Health Check (kein API-Key benÃ¶tigt)

**Beispiele fÃ¼r API-Anfragen:**

**Linux/macOS (curl):**

```bash
curl -X GET \
  'http://localhost:5000/api/v1/fetch-text?url=https://www.example.com&stopwords=example,test&css_selectors={"title": "h1"}' \
  -H 'X-API-Key: IhrAPIKey'
```

**Windows (PowerShell - Invoke-WebRequest):**

```powershell
$API_KEY = "IhrAPIKey" # Ersetzen Sie dies mit Ihrem API-Key
$API_HOST = "http://localhost:5000"

$response = Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-text?url=https://www.example.com&stopwords=example,test&css_selectors={\"title\": \"h1\"}" `
    -Headers @{'X-API-Key' = $API_KEY}

# Antwortinhalt als JSON ausgeben (optional)
$response.Content | ConvertFrom-Json
```

**Weitere Windows/PowerShell Beispiele (Invoke-WebRequest):**

*   **Links von einer Webseite abrufen:**

    ```powershell
    $API_KEY = "IhrAPIKey"
    $API_HOST = "http://localhost:5000"
    $BASE_URL = "https://www.example.com"

    $links_response = Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-links?url=$BASE_URL" `
        -Headers @{'X-API-Key' = $API_KEY} | ConvertFrom-Json

    # Links ausgeben (optional)
    $links_response.data.links
    ```

*   **HTML-Inhalt abrufen und in Datei speichern:**

    ```powershell
    $API_KEY = "IhrAPIKey"
    $API_HOST = "http://localhost:5000"
    $URL_TO_FETCH = "https://www.example.com"

    Invoke-WebRequest -Uri "$API_HOST/api/v1/fetch-html?url=$URL_TO_FETCH&save_file=true" `
        -Headers @{'X-API-Key' = $API_KEY}
    ```

### 4.3 Streamlit Admin-OberflÃ¤che ğŸ–¥ï¸

Starten Sie die Admin-OberflÃ¤che:

```bash
python app.py --streamlit
```

Zugriff Ã¼ber `http://localhost:8501`.

**BenutzeroberflÃ¤che:**

1.  **API-Key Eingabe** ğŸ”‘
2.  **Geplante Tasks Ãœbersicht** ğŸ“ (mit Details und Aktionen)
3.  **Neuen Task hinzufÃ¼gen**â• (Formular)

**Bedienungshinweise:**

*   API-Key zu Beginn eingeben.
*   "Geplante Tasks" Ãœbersicht fÃ¼r Task-Management.
*   "Neuen Task hinzufÃ¼gen" fÃ¼r neue Tasks.
*   ZeitplÃ¤ne und CSS-Selektoren korrekt formatieren.
*   Seite wird nach Task-Ã„nderungen neu geladen.

## 5. Funktionsbeschreibung

### 5.1 Geplante Tasks erstellen (API und Admin-OberflÃ¤che) â•ğŸ“

**Request Body (JSON) fÃ¼r API Task-Erstellung:**

```json
{
  "url": "https://www.example.com",
  "schedule_time": "tÃ¤glich um 08:00",
  "text_only": false,
  "stopwords": "example,test",
  "css_selectors": "{\"title\": \"h1\"}",
  "save_file": true,
  "processing_function_path": "custom_processing.py"
}
```

**Parameter:** `url`, `schedule_time`, `text_only`, `stopwords`, `css_selectors`, `save_file`, `processing_function_path`.

### 5.2 Geplante Tasks aktualisieren (API und Admin-OberflÃ¤che) ğŸ”„ğŸ“

**Request Body (JSON) fÃ¼r API Task-Aktualisierung:**

```json
{
  "schedule_time": "stÃ¼ndlich",
  "stopwords": "neue,stopwÃ¶rter"
}
```

**Parameter:** `task_id` (Pfadparameter), Request Body (JSON) mit zu aktualisierenden Feldern.

### 5.3 Geplante Tasks lÃ¶schen (API und Admin-OberflÃ¤che) âŒğŸ“

**Parameter:** `task_id` (Pfadparameter).

### 5.4 Geplante Tasks manuell ausfÃ¼hren (API und Admin-OberflÃ¤che) â–¶ï¸ğŸ“

**Parameter:** `task_id` (Pfadparameter).

### 5.5 Webseiteninhalt abrufen und extrahieren (API und Kommandozeile) ğŸŒâ¡ï¸ğŸ“„

**Prozessablauf:**

1.  URL-Validierung âœ…
2.  Cache-PrÃ¼fung ğŸ—„ï¸
3.  Webseitenabruf (Selenium) ğŸŒ
4.  HTML-Parsing (Beautiful Soup) ğŸ¥£
5.  Datenextraktion (Text, Titel, Meta-Description, H1-Headings, Keywords, CSS-Daten) ğŸ“„
6.  Benutzerdefinierte Datenverarbeitung (optional) âš™ï¸
7.  Datenbank-Speicherung ğŸ’¾
8.  Datei-Speicherung (optional) ğŸ—‚ï¸
9.  Antwortgenerierung (API) / Ausgabe (Kommandozeile) ğŸ“¤

### 5.6 Keyword-Extraktion ğŸ”‘ğŸ§®

*   Textvorverarbeitung, Stopwortfilterung, alphabetische Filterung, WorthÃ¤ufigkeitszÃ¤hlung, Top-N Keywords.

### 5.7 CSS-Datenextraktion ğŸ§±

*   Einfache und konfigurierte Selektoren (mit `selector`, `type`, `cleanup`).
*   SicherheitsprÃ¼fung fÃ¼r CSS-Selektoren.

### 5.8 Benutzerdefinierte Datenverarbeitung âš™ï¸

*   `process_data(data)` Funktion in Python-Datei definieren.
*   Pfad zur Datei in Programm/Task konfigurieren.
*   Sicherheitswarnung beachten.âš ï¸

## 6. Beispielhafte AnwendungsfÃ¤lle

*   Einmaliges Scrapen Ã¼ber Kommandozeile ğŸš€
*   RegelmÃ¤ÃŸiges Scrapen mit geplantem Task â±ï¸
*   Extrahieren von Produktinformationen mit CSS-Selektoren ğŸ›ï¸
*   Datenanalyse mit benutzerdefinierter Processing-Funktion ğŸ“Š
*   Abrufen von Links Ã¼ber API ğŸ”—

## 7. Fehlerbehebung

**HÃ¤ufige Fehlermeldungen und LÃ¶sungen:**

*   "UngÃ¼ltige URL" âŒğŸŒ
*   "Webseiteninhalt konnte nicht abgerufen werden" âŒ
*   "API-Key fehlt oder ist ungÃ¼ltig." âŒğŸ”‘
*   "Rate Limit Ã¼berschritten. Bitte warten Sie eine Minute." â³
*   "UngÃ¼ltiges JSON-Format fÃ¼r CSS-Selektoren." âŒğŸ§±
*   "UngÃ¼ltiger Pfad zur Processing-Funktion" âŒâš™ï¸
*   "Fehler beim Speichern in die Datenbank" âŒğŸ’¾
*   "Fehler in der Datenverarbeitungsfunktion" âŒâš™ï¸
*    "Kritischer Datenbankfehler im Scheduled Mode. Programm wird beendet." â˜ ï¸ğŸ’¾

**Log-Level Konfiguration:**

Konfigurierbar in `config.yaml` unter `log_level`.

**VerfÃ¼gbare Log-Level:**

*   `DEBUG` (detaillierteste Protokollierung) ğŸ›
*   `INFO` (Standard) â„¹ï¸
*   `WARNING` âš ï¸
*   `ERROR` âŒ
*   `CRITICAL` â˜ ï¸

**Beispiel `config.yaml` fÃ¼r `DEBUG` Log-Level:**

```yaml
log_level: DEBUG
```

## 8. FAQ (HÃ¤ufig gestellte Fragen)

**F: Wie konfiguriere ich API-Keys?**

A: API-Keys kÃ¶nnen in der `config.yaml` Datei unter `api_keys` als Liste von Strings oder sicherer Ã¼ber Umgebungsvariablen (siehe Abschnitt 3.1) konfiguriert werden.

**F: Wie Ã¤ndere ich das Rate Limit der API?**

A: Das Rate Limit (maximale Anfragen pro Minute) kann in der `config.yaml` Datei unter `rate_limit_requests_per_minute` konfiguriert werden.

**F: Wie lange werden Webseiten im Cache gespeichert?**

A: Die GÃ¼ltigkeitsdauer des Caches (in Sekunden) kann in der `config.yaml` Datei unter `cache_expiry_seconds` konfiguriert werden. StandardmÃ¤ÃŸig sind es 600 Sekunden (10 Minuten).

**F: Wie kann ich geplante Tasks verwalten?**

A: Geplante Tasks kÃ¶nnen Ã¼ber die Streamlit Admin-OberflÃ¤che (empfohlen) oder Ã¼ber die Web-API verwaltet werden (erstellen, aktualisieren, lÃ¶schen, auflisten, manuell ausfÃ¼hren, Status abrufen).

**F: Wo werden die gescrapten Daten gespeichert?**

A: Die gescrapten Daten werden in einer SQLite-Datenbank gespeichert. Der Pfad zur Datenbankdatei kann in der `config.yaml` Datei unter `database_file` konfiguriert werden.

**F: Kann ich nur Textinhalte extrahieren?**

A: Ja, Sie kÃ¶nnen nur Textinhalte extrahieren, indem Sie die Option `--text` in der Kommandozeile verwenden, `text_only=true` im Request Body fÃ¼r die API-Endpunkte `/fetch-html` und `/fetch-text` setzen oder die Checkbox "Nur Text extrahieren" in der Streamlit Admin-OberflÃ¤che aktivieren.

**F: Wie kann ich benutzerdefinierte StopwÃ¶rter verwenden?**

A: Benutzerdefinierte StopwÃ¶rter kÃ¶nnen als kommagetrennte Liste Ã¼ber die Option `--stopwords` in der Kommandozeile, den Parameter `stopwords` in den API-Endpunkten `/fetch-html` und `/fetch-text` oder das Textfeld "StopwÃ¶rter" in der Streamlit Admin-OberflÃ¤che angegeben werden.

**F: Sind CSS-Selektoren sicher zu verwenden?**

A: WebCrawler-Pro implementiert SicherheitsprÃ¼fungen fÃ¼r CSS-Selektoren, um potenziell unsichere Selektoren zu erkennen und zu verhindern. Dennoch sollten Sie bei der Verwendung von CSS-Selektoren Vorsicht walten lassen und nur vertrauenswÃ¼rdige Selektoren verwenden.

**F: Sind benutzerdefinierte Processing-Funktionen sicher?**

A: Benutzerdefinierte Processing-Funktionen kÃ¶nnen beliebigen Python-Code ausfÃ¼hren. Verwenden Sie diese Funktion nur mit Bedacht und stellen Sie sicher, dass Sie nur vertrauenswÃ¼rdigen Code ausfÃ¼hren, um Sicherheitsrisiken zu vermeiden. WebCrawler-Pro validiert den Pfad zur Processing-Funktion, um unsichere Pfade zu verhindern.

**F: UnterstÃ¼tzt WebCrawler-Pro JavaScript-Rendering?**

A: Ja, WebCrawler-Pro verwendet Selenium und ChromeDriver, um Webseiten abzurufen, was das Rendering von JavaScript-Inhalten ermÃ¶glicht.

## 9. Glossar

*   **API (Application Programming Interface):** ğŸŒ Eine Schnittstelle, die es Softwareanwendungen ermÃ¶glicht, miteinander zu kommunizieren. Im Kontext von WebCrawler-Pro ermÃ¶glicht die API den programmatischen Zugriff auf Web-Scraping-FunktionalitÃ¤ten.
*   **CSS-Selektor:** ğŸ§± Ein Muster, das verwendet wird, um HTML-Elemente auf einer Webseite auszuwÃ¤hlen und zu formatieren oder Daten aus diesen Elementen zu extrahieren.
*   **ChromeDriver:** ğŸŒ Ein separates ausfÃ¼hrbares Programm, das von Selenium verwendet wird, um Chrome-Browser zu steuern.
*   **JSON (JavaScript Object Notation):** ğŸ“„ Ein leichtgewichtiges Datenformat, das fÃ¼r den Datenaustausch im Web verwendet wird.
*   **Rate Limiting:** â³ Eine Technik zur Begrenzung der Anzahl von Anfragen, die ein Benutzer oder eine Anwendung innerhalb eines bestimmten Zeitraums an eine API senden kann. Dies dient dem Schutz vor Ãœberlastung und Missbrauch.
*   **Caching:** ğŸ—„ï¸ Eine Technik zur Speicherung hÃ¤ufig abgerufener Daten (z.B. Webseiteninhalte) in einem temporÃ¤ren Speicher (Cache), um den Zugriff zu beschleunigen und die Last auf den ursprÃ¼nglichen Datenquelle zu reduzieren.
*   **Scheduled Task (Geplanter Task):** â±ï¸ğŸ“ Eine Aufgabe, die automatisch zu einem vordefinierten Zeitpunkt oder in regelmÃ¤ÃŸigen Intervallen ausgefÃ¼hrt wird. Im Kontext von WebCrawler-Pro sind geplante Tasks Web-Scraping-Aufgaben, die automatisch nach Zeitplan ausgefÃ¼hrt werden.
*   **Selenium:** ğŸŒ Ein Framework fÃ¼r die Automatisierung von Webbrowsern. WebCrawler-Pro verwendet Selenium, um Webseiten dynamisch abzurufen und JavaScript-Inhalte zu rendern.

## 10. Kontakt und Support

**E-Mail:** ğŸ“§ support@webcrawler-pro.example.com (Platzhalter E-Mail Adresse)

**Webseite:** ğŸŒ www.webcrawler-pro.example.com (Platzhalter Webseite)

Bitte beschreiben Sie Ihr Problem/Anfrage detailliert.


