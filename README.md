# ğŸš€ **WebCrawler-Pro**

## ğŸ“Œ **EinfÃ¼hrung**
WebCrawler-Pro ist ein leistungsstarkes Werkzeug zur **automatischen Extraktion, Verarbeitung und Bereitstellung von Web-Daten** Ã¼ber eine API. Es kombiniert **Web-Scraping, Datenverarbeitung, API-Integration, Sicherheit und Task-Planung** in einem einzigen System. Die geplante Task-AusfÃ¼hrung und das Monitoring erfolgen direkt Ã¼ber die Datenbank.  ZusÃ¤tzlich bietet WebCrawler-Pro eine **intuitive Web-OberflÃ¤che mit Streamlit**, um geplante Tasks komfortabel zu verwalten.

### ğŸ”¹ **Hauptfunktionen**
âœ… **Web-Scraping mit Selenium** (automatisiertes Abrufen von Webseiteninhalten)
âœ… **Gezielte Datenextraktion mit CSS-Selektoren** (inkl. Typkonvertierung & Datenbereinigung)
âœ… **Datenverarbeitung mit benutzerdefinierten Funktionen** (`processing.py`)
âœ… **RESTful API zur Bereitstellung der Daten** (mit detaillierten Fehlermeldungen)
âœ… **Automatisierte Task-Planung aus der Datenbank** (mit Monitoring & manueller AusfÃ¼hrung Ã¼ber die API)
âœ… **API-Authentifizierung per API-Key** ğŸ”‘
âœ… **Ratenbegrenzung zum Schutz vor Missbrauch** ğŸ›¡ï¸
âœ… **Caching zur Leistungssteigerung** âš¡
âœ… **Datenbankintegration mit SQLite** ğŸ—„ï¸ (mit Transaktionssicherheit)
âœ… **Erweiterbare SicherheitsmaÃŸnahmen gegen Path Traversal & CSS-Injection**
âœ… **Monitoring fÃ¼r geplante Tasks und API-Status Ã¼ber API-Endpunkte** ğŸ“Š (inkl. Start-/Endzeiten, Logs, Fehlerberichte, letzter/nÃ¤chster AusfÃ¼hrungszeit)
âœ… **Einfache Konfiguration Ã¼ber YAML-Dateien & Umgebungsvariablen** âš™ï¸
âœ… **Web-OberflÃ¤che mit Streamlit zur Taskverwaltung** ğŸ–¥ï¸

ğŸ“– Diese Dokumentation beschreibt die **Installation, Konfiguration und Nutzung** des Programms.

---

## ğŸ”§ **1. Installation**
### ğŸ“‚ **1.1 Voraussetzungen**
ğŸ“Œ **Erforderliche Software:**
- ğŸ **Python 3.7 oder hÃ¶her** *(empfohlen: 3.9+)*
- ğŸ“¦ **pip** *(Python-Paketmanager, sollte mit Python installiert sein)*
- ğŸŒ **Google Chrome + ChromeDriver** *(fÃ¼r Selenium-basiertes Scraping)*
- ğŸ§  **NLTK Data:** *(FÃ¼r Keyword-Extraktion: `python -m nltk.downloader stopwords`)*
- ğŸ–¥ï¸ **Streamlit:** *(FÃ¼r die Web-OberflÃ¤che: `pip install streamlit`)*

### ğŸ“¥ **1.2 AbhÃ¤ngigkeiten installieren**
FÃ¼hre folgenden Befehl aus, um alle benÃ¶tigten Pakete zu installieren:
```bash
pip install -r requirements.txt
```


---

## âš™ï¸ **2. Konfiguration**
Das Programm wird Ã¼ber **YAML-Dateien & Umgebungsvariablen** konfiguriert:
- ğŸ“„ **`config.yaml`** â†’ EnthÃ¤lt Einstellungen fÃ¼r Scraping, API, Datenbank & Sicherheit.
- ğŸ”‘ **`.env` Datei** â†’ Speichert API-Keys & andere sensible Informationen. *(Umgebungsvariablen haben Vorrang vor `config.yaml`.)*  API-Keys kÃ¶nnen auch direkt in `config.yaml` unter `api_keys` eingetragen werden.

### ğŸ›  **2.1 `config.yaml` (Beispiel)**
```yaml
scraping:
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
  timeout: 10

database:
  type: sqlite
  path: "data/webcrawler.db"

security:
  api_key_required: true
  rate_limit: 100
```

### ğŸ”‘ **2.2 `.env` Datei (Beispiel)**
```ini
API_KEY_1=mein_geheimer_api_key_1
API_KEY_2=mein_geheimer_api_key_2
API_KEY_3=mein_geheimer_api_key_3
```

---

## â–¶ï¸ **3. Nutzung**
### ğŸŒ **3.1 API-Modus** *(Startet den API-Server)*
```bash
python app.py --api
```

### ğŸ•µï¸ **3.2 Kommandozeilenmodus** *(Einzelnes Scraping ausfÃ¼hren)*
```bash
python app.py https://example.com [Optionen]
```

### â³ **3.3 Geplanter Modus** *(Automatische Tasks aus der Datenbank ausfÃ¼hren)*
```bash
python app.py
```
ğŸ“Œ **Hinweis:** Geplante Tasks werden aus der Datenbank (`webdata.db`, konfigurierbar in `config.yaml`) geladen und ausgefÃ¼hrt. 
---


## ğŸ›  **4. Benutzerdefinierte Datenverarbeitung (`processing.py`)**
ğŸ“Œ **ErmÃ¶glicht benutzerdefinierte Verarbeitung gescrapter Daten.**

**Beispiel:**
```python
def process_data(data: dict) -> dict:
    """
    Verarbeitet gescrapte Daten. Muss ein Dictionary zurÃ¼ckgeben.
    Falls `None` zurÃ¼ckgegeben wird, erscheint eine Warnung im Log.
    """
    if not isinstance(data, dict):
        return None

    processed_data = {k: v.strip() if isinstance(v, str) else v for k, v in data.items()}
    return processed_data
```

---

## ğŸ”’ **5. Sicherheitshinweise**
ğŸ“Œ **Schutzmechanismen:**
- ğŸ”‘ **API-Key-Authentifizierung** *(API-Endpunkte erfordern einen API-Key im Header)*
- ğŸ›¡ **Ratenbegrenzung** *(Maximal 100 Anfragen pro Minute, konfigurierbar in `config.yaml`)*
- ğŸš¨ **Path Traversal-Schutz** *(Verhindert unautorisierten Zugriff auf Dateien)*
- ğŸ” **CSS-Selektor-Validierung** *(Unsichere Selektoren werden ignoriert & protokolliert)*

**Beispiel fÃ¼r ein Sicherheitslog:**
```sh
2025-03-09 14:43:19,238 - WARNING - Unsicherer CSS-Selektor erkannt: div[onclick*=alert]
```

---

## ğŸ“Š **6. Fehlerbehandlung & Logging**
ğŸ“Œ **Wo werden Fehler protokolliert?**
- ğŸ–¥ **Konsolenausgabe** *(Standard, fÃ¼r schnelle Fehleranalyse)*
- ğŸ—‚ **Log-Datei `logs/webcrawler.log`** *(falls aktiviert)*

**Log-Level:**
- âœ… **INFO** â†’ Allgemeine Statusmeldungen
- âš ï¸ **WARNING** â†’ Sicherheitswarnungen
- âŒ **ERROR** â†’ Kritische Fehler


## ğŸ“¡ 7. API-Endpunkte
ğŸ“Œ **Ãœbersicht der wichtigsten API-Endpunkte (aktualisiert):**
- ğŸŒ `/api/v1/` â†’ API Root mit Ãœbersicht aller Endpunkte
- ğŸ“„ `/api/v1/fetch-html` â†’ HTML-Inhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ“„ `/api/v1/fetch-text` â†’ Textinhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ”„ `/api/v1/scheduled-tasks` â†’ Aufgabenverwaltung (GET, POST, PUT, DELETE)
- ğŸ”„ `/api/v1/scheduled-tasks/<task_id>` â†’ Einzelne Task verwalten (GET, PUT, DELETE)
- ğŸ“Š `/api/v1/scheduled-tasks/status` â†’ Status aller geplanten Tasks
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/status` â†’ Status eines spezifischen Tasks  (inkl. letzter/nÃ¤chster AusfÃ¼hrungszeit und Fehlermeldungen)
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/run` â†’ Manuelles AusfÃ¼hren eines Tasks
- âœ… `/api/v1/health` â†’ API Health Check


ğŸ” **Alle API-Endpunkte erfordern API-Authentifizierung!**

---
## **WeboberflÃ¤che**

Mit dem Befehl `streamlit run app.py -- --streamlit` starten Sie die WeboberflÃ¤che zur bequemen Verwaltung Ihrer Webcrawling-Tasks.  Sie benÃ¶tigen hierfÃ¼r einen gÃ¼ltigen API-Key.

**API-Key Eingabe:**

Nach dem Start der WeboberflÃ¤che werden Sie aufgefordert, Ihren API-Key einzugeben.  Dieser dient zur Authentifizierung und Autorisierung des Zugriffs auf die Funktionen der WeboberflÃ¤che. Geben Sie Ihren API-Key in das dafÃ¼r vorgesehene Feld ein und bestÃ¤tigen Sie mit Enter. Ein ungÃ¼ltiger API Key fÃ¼hrt zu einer Fehlermeldung.

**Anzeige geplanter Tasks:**

Die WeboberflÃ¤che listet alle geplanten Tasks Ã¼bersichtlich auf. FÃ¼r jeden Task werden folgende Informationen angezeigt:

* **Task ID:** Eindeutige Identifikationsnummer des Tasks.
* **URL:** Die zu crawlende Webseite.
* **Zeitplan:**  Definiert, wann der Task ausgefÃ¼hrt wird (z.B. stÃ¼ndlich, tÃ¤glich um 10:00, alle 30 Minuten).
* **Nur Text:**  Gibt an, ob nur der Textinhalt oder der gesamte HTML-Code extrahiert werden soll.
* **StopwÃ¶rter:**  Auflistung der StopwÃ¶rter, die bei der Keyword-Extraktion ignoriert werden.
* **CSS-Selektoren:**  Die verwendeten CSS-Selektoren zur gezielten Datenextraktion.
* **Datei speichern:**  Gibt an, ob die extrahierten Daten zusÃ¤tzlich zur Datenbank in einer Datei gespeichert werden sollen.
* **Verarbeitungsfunktion:**  Pfad zur benutzerdefinierten Verarbeitungsfunktion (optional).
* **Status:**  Aktueller Status des Tasks (z.B. `pending`, `running`, `success`, `failure`).
* **Letzte AusfÃ¼hrung:**  Zeitstempel der letzten AusfÃ¼hrung.
* **NÃ¤chste AusfÃ¼hrung:**  Zeitstempel der nÃ¤chsten geplanten AusfÃ¼hrung.
* **Fehlermeldung:**  Anzeige von etwaigen Fehlermeldungen bei der AusfÃ¼hrung des Tasks.


**Aktionen fÃ¼r jeden Task:**

* **LÃ¶schen:**  Ãœber den Button "Task [ID] lÃ¶schen" kann ein geplanter Task entfernt werden.
* **Sofort ausfÃ¼hren:** Mit dem Button "Task [ID] sofort ausfÃ¼hren" kann ein Task unabhÃ¤ngig vom Zeitplan manuell gestartet werden.  Der Status aktualisiert sich nach der AusfÃ¼hrung.

**HinzufÃ¼gen eines neuen Tasks:**

Im Bereich "Neuen Task hinzufÃ¼gen" kÃ¶nnen Sie neue Crawling-Tasks erstellen.  FÃ¼llen Sie die folgenden Felder aus:

* **URL:**  Die URL der zu crawlenden Webseite.
* **Zeitplan:**  Definieren Sie den AusfÃ¼hrungszeitplan des Tasks. GÃ¼ltige Formate sind: "stÃ¼ndlich", "tÃ¤glich um HH:MM" oder "alle X minuten".
* **Nur Text extrahieren:** Aktivieren Sie diese Option, um nur den Textinhalt der Webseite zu extrahieren.
* **StopwÃ¶rter:**  Geben Sie optional eine kommagetrennte Liste von StopwÃ¶rtern ein.
* **CSS-Selektoren:** FÃ¼gen Sie optional CSS-Selektoren im JSON-Format hinzu, um bestimmte Daten von der Webseite zu extrahieren.
* **Datei speichern:**  Aktivieren Sie diese Option, um die extrahierten Daten in einer Datei zu speichern.
* **Verarbeitungsfunktion:** Geben Sie optional den Pfad zu einer benutzerdefinierten Verarbeitungsfunktion an.


Nach dem AusfÃ¼llen der Felder klicken Sie auf "Task hinzufÃ¼gen", um den neuen Task zu speichern und zu planen.  Die WeboberflÃ¤che aktualisiert sich automatisch und zeigt den neu hinzugefÃ¼gten Task an.  Fehler bei der Eingabe werden direkt angezeigt.

```
```bash
streamlit run app.py -- --streamlit
```

![image](https://github.com/user-attachments/assets/d2b6b9aa-ebaa-4450-8870-0096207fb2e1)

---
---
## **Hier sind einige Beispieleingaben fÃ¼r die Streamlit-WeboberflÃ¤che deines WebCrawler-Pro**

**Beispiel 1: Einfacher Webseiten-Crawl**

* **URL:** `https://www.example.com`
* **Zeitplan:** `tÃ¤glich um 10:00`
* **Nur Text extrahieren:** (deaktiviert)
* **StopwÃ¶rter:**
* **CSS-Selektoren:**
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:**


Dieser Task crawlt tÃ¤glich um 10:00 Uhr die Webseite `https://www.example.com` und speichert den gesamten HTML-Inhalt in einer Datei.  Es werden keine StopwÃ¶rter, CSS-Selektoren oder Verarbeitungsfunktionen verwendet.

**Beispiel 2: Text-Extraktion mit StopwÃ¶rtern**

* **URL:** `https://www.wikipedia.org`
* **Zeitplan:** `alle 60 minuten`
* **Nur Text extrahieren:** (aktiviert)
* **StopwÃ¶rter:** `und, die, der, das, ist`
* **CSS-Selektoren:**
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:**


Dieser Task extrahiert stÃ¼ndlich den Textinhalt von `https://www.wikipedia.org`. Die angegebenen StopwÃ¶rter werden bei der Keyword-Extraktion ignoriert. Der extrahierte Text wird in einer Datei gespeichert.

**Beispiel 3: Datenextraktion mit CSS-Selektoren**

* **URL:** `https://www.amazon.de/`
* **Zeitplan:** `stÃ¼ndlich`
* **Nur Text extrahieren:** (deaktiviert)
* **StopwÃ¶rter:**
* **CSS-Selektoren:**
```json
{
  "product_title": "h2.a-size-mini a.a-link-normal span",
  "product_price": "span.a-price span.a-offscreen"
}
```
* **Datei speichern:** (deaktiviert)
* **Verarbeitungsfunktion:** `./PROCESSING_FUNCTIONS_DIR/#1 custom_processing.py`


Dieser Task crawlt stÃ¼ndlich Amazon und extrahiert Produkttitel und -preise mithilfe der angegebenen CSS-Selektoren. Die extrahierten Daten werden mit der angegebenen Verarbeitungsfunktion weiterverarbeitet und in der Datenbank gespeichert. Beachte, dass der Pfad zur Verarbeitungsfunktion relativ zum AusfÃ¼hrungsverzeichnis des Crawlers sein muss.


**Beispiel 4:  Kombination aller Funktionen**

* **URL:** `https://www.heise.de`
* **Zeitplan:** `tÃ¤glich um 06:00`
* **Nur Text extrahieren:** (aktiviert)
* **StopwÃ¶rter:** `mit, von, am, im`
* **CSS-Selektoren:** `{"article_title": "h2 a"}`
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:** `./PROCESSING_FUNCTIONS_DIR/#2 custom_processing.py` (oder ein anderer gÃ¼ltiger Pfad)


Dieser Task kombiniert alle verfÃ¼gbaren Funktionen. Er crawlt tÃ¤glich um 6:00 Uhr heise.de, extrahiert den Textinhalt, filtert die angegebenen StopwÃ¶rter, extrahiert Artikeltitel mithilfe des CSS-Selektors, speichert den Textinhalt in einer Datei und verarbeitet die Daten mit der angegebenen Funktion.

**Wichtig:**

* **GÃ¼ltige Pfade fÃ¼r Verarbeitungsfunktionen:** Achte darauf, dass die Pfade zu den Verarbeitungsfunktionen korrekt und relativ zum AusfÃ¼hrungsverzeichnis des Crawlers angegeben sind.  Die Beispiele oben gehen davon aus, dass sich die `custom_processing.py` Dateien im Verzeichnis `PROCESSING_FUNCTIONS_DIR` befinden.
* **JSON-Format fÃ¼r CSS-Selektoren:** Die CSS-Selektoren mÃ¼ssen in einem gÃ¼ltigen JSON-Format angegeben werden.
* **Testen:** Teste die Eingaben grÃ¼ndlich, um sicherzustellen, dass sie die gewÃ¼nschten Ergebnisse liefern.
---
## ğŸ **9. WebCrawler-Pro**
âœ… **Automatisierte Task-Planung direkt aus der Datenbank**
âœ… **Erweiterbare Datenverarbeitung durch `processing.py`**
âœ… **Detaillierte Logging- & SicherheitsmaÃŸnahmen**
âœ… **Einfache Konfiguration Ã¼ber `config.yaml` & `.env`**

ğŸš€ **WebCrawler-Pro ist die ideale LÃ¶sung fÃ¼r produktives, sicheres und flexibles Web-Scraping!**

