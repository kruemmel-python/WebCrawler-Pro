# ğŸš€ **WebCrawler-Pro**

## ğŸ“Œ **EinfÃ¼hrung**
WebCrawler-Pro ist ein leistungsstarkes Werkzeug zur **automatischen Extraktion, Verarbeitung und Bereitstellung von Web-Daten** Ã¼ber eine API. Es kombiniert **Web-Scraping, Datenverarbeitung, API-Integration, Sicherheit und Task-Planung** in einem einzigen System. Die geplante Task-AusfÃ¼hrung und das Monitoring erfolgen direkt Ã¼ber die Datenbank.

### ğŸ”¹ **Hauptfunktionen**
âœ… **Web-Scraping mit Selenium** (automatisiertes Abrufen von Webseiteninhalten)
âœ… **Gezielte Datenextraktion mit CSS-Selektoren** (inkl. Typkonvertierung & Datenbereinigung)
âœ… **Datenverarbeitung mit benutzerdefinierten Funktionen** (`processing.py`)
âœ… **RESTful API zur Bereitstellung der Daten** (mit detaillierten Fehlermeldungen)
âœ… **Automatisierte Task-Planung aus der Datenbank** (mit Monitoring & manueller AusfÃ¼hrung Ã¼ber die API)
âœ… **API-Authentifizierung per API-Key** ğŸ”‘
âœ… **Ratenbegrenzung zum Schutz vor Missbrauch** ğŸ›¡ï¸
âœ… **Caching zur Leistungssteigerung** âš¡
âœ… **Datenbankintegration mit SQLite** ğŸ—„ï¸ (mit Transaktionssicherheit)
âœ… **Erweiterbare SicherheitsmaÃŸnahmen gegen Path Traversal & CSS-Injection**
âœ… **Monitoring fÃ¼r geplante Tasks und API-Status Ã¼ber API-Endpunkte** ğŸ“Š (inkl. Start-/Endzeiten, Logs, Fehlerberichte, letzter/nÃ¤chster AusfÃ¼hrungszeit)
âœ… **Einfache Konfiguration Ã¼ber YAML-Dateien & Umgebungsvariablen** âš™ï¸

ğŸ“– Diese Dokumentation beschreibt die **Installation, Konfiguration und Nutzung** des Programms.

---

## ğŸ”§ **1. Installation**
### ğŸ“‚ **1.1 Voraussetzungen**
ğŸ“Œ **Erforderliche Software:**
- ğŸ **Python 3.7 oder hÃ¶her** *(empfohlen: 3.9+)*
- ğŸ“¦ **pip** *(Python-Paketmanager, sollte mit Python installiert sein)*
- ğŸŒ **Google Chrome + ChromeDriver** *(fÃ¼r Selenium-basiertes Scraping)*
- ğŸ§  **NLTK Data:** *(FÃ¼r Keyword-Extraktion: `python -m nltk.downloader stopwords`)*

### ğŸ“¥ **1.2 AbhÃ¤ngigkeiten installieren**
FÃ¼hre folgenden Befehl aus, um alle benÃ¶tigten Pakete zu installieren:
```bash
pip install -r requirements.txt
```

---

## âš™ï¸ **2. Konfiguration**
Das Programm wird Ã¼ber **YAML-Dateien & Umgebungsvariablen** konfiguriert:
- ğŸ“„ **`config.yaml`** â†’ EnthÃ¤lt Einstellungen fÃ¼r Scraping, API, Datenbank & Sicherheit.
- ğŸ”‘ **`.env` Datei** â†’ Speichert API-Keys & andere sensible Informationen. *(Umgebungsvariablen haben Vorrang vor `config.yaml`.)*  API-Keys kÃ¶nnen auch direkt in `config.yaml` unter `api_keys` eingetragen werden.

### ğŸ›  **2.1 `config.yaml` (Beispiel)**
```yaml
scraping:
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
  timeout: 10

database:
  type: sqlite
  path: "data/webcrawler.db"

security:
  api_key_required: true
  rate_limit: 100
```

### ğŸ”‘ **2.2 `.env` Datei (Beispiel)**
```ini
API_KEY_1=mein_geheimer_api_key_1
API_KEY_2=mein_geheimer_api_key_2
API_KEY_3=mein_geheimer_api_key_3
```

---

## â–¶ï¸ **3. Nutzung**
### ğŸŒ **3.1 API-Modus** *(Startet den API-Server)*
```bash
python app.py --api
```

### ğŸ•µï¸ **3.2 Kommandozeilenmodus** *(Einzelnes Scraping ausfÃ¼hren)*
```bash
python app.py https://example.com [Optionen]
```

### â³ **3.3 Geplanter Modus** *(Automatische Tasks aus der Datenbank ausfÃ¼hren)*
```bash
python app.py
```
ğŸ“Œ **Hinweis:** Geplante Tasks werden aus der Datenbank (`webdata.db`, konfigurierbar in `config.yaml`) geladen und ausgefÃ¼hrt. 
---


## ğŸ›  **4. Benutzerdefinierte Datenverarbeitung (`processing.py`)**
ğŸ“Œ **ErmÃ¶glicht benutzerdefinierte Verarbeitung gescrapter Daten.**

**Beispiel:**
```python
def process_data(data: dict) -> dict:
    """
    Verarbeitet gescrapte Daten. Muss ein Dictionary zurÃ¼ckgeben.
    Falls `None` zurÃ¼ckgegeben wird, erscheint eine Warnung im Log.
    """
    if not isinstance(data, dict):
        return None

    processed_data = {k: v.strip() if isinstance(v, str) else v for k, v in data.items()}
    return processed_data
```

---

## ğŸ”’ **5. Sicherheitshinweise**
ğŸ“Œ **Schutzmechanismen:**
- ğŸ”‘ **API-Key-Authentifizierung** *(API-Endpunkte erfordern einen API-Key im Header)*
- ğŸ›¡ **Ratenbegrenzung** *(Maximal 100 Anfragen pro Minute, konfigurierbar in `config.yaml`)*
- ğŸš¨ **Path Traversal-Schutz** *(Verhindert unautorisierten Zugriff auf Dateien)*
- ğŸ” **CSS-Selektor-Validierung** *(Unsichere Selektoren werden ignoriert & protokolliert)*

**Beispiel fÃ¼r ein Sicherheitslog:**
```sh
2025-03-09 14:43:19,238 - WARNING - Unsicherer CSS-Selektor erkannt: div[onclick*=alert]
```

---

## ğŸ“Š **6. Fehlerbehandlung & Logging**
ğŸ“Œ **Wo werden Fehler protokolliert?**
- ğŸ–¥ **Konsolenausgabe** *(Standard, fÃ¼r schnelle Fehleranalyse)*
- ğŸ—‚ **Log-Datei `logs/webcrawler.log`** *(falls aktiviert)*

**Log-Level:**
- âœ… **INFO** â†’ Allgemeine Statusmeldungen
- âš ï¸ **WARNING** â†’ Sicherheitswarnungen
- âŒ **ERROR** â†’ Kritische Fehler


## ğŸ“¡ 7. API-Endpunkte
ğŸ“Œ **Ãœbersicht der wichtigsten API-Endpunkte (aktualisiert):**
- ğŸŒ `/api/v1/` â†’ API Root mit Ãœbersicht aller Endpunkte
- ğŸ“„ `/api/v1/fetch-html` â†’ HTML-Inhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ“„ `/api/v1/fetch-text` â†’ Textinhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ”„ `/api/v1/scheduled-tasks` â†’ Aufgabenverwaltung (GET, POST, PUT, DELETE)
- ğŸ”„ `/api/v1/scheduled-tasks/<task_id>` â†’ Einzelne Task verwalten (GET, PUT, DELETE)
- ğŸ“Š `/api/v1/scheduled-tasks/status` â†’ Status aller geplanten Tasks
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/status` â†’ Status eines spezifischen Tasks  (inkl. letzter/nÃ¤chster AusfÃ¼hrungszeit und Fehlermeldungen)
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/run` â†’ Manuelles AusfÃ¼hren eines Tasks
- âœ… `/api/v1/health` â†’ API Health Check


ğŸ” **Alle API-Endpunkte erfordern API-Authentifizierung!**

---
**WeboberflÃ¤che**
```sh
streamlit run app.py -- --streamlit
```

![image](https://github.com/user-attachments/assets/d2b6b9aa-ebaa-4450-8870-0096207fb2e1)

---

## ğŸ **9. WebCrawler-Pro**
âœ… **Automatisierte Task-Planung direkt aus der Datenbank**
âœ… **Erweiterbare Datenverarbeitung durch `processing.py`**
âœ… **Detaillierte Logging- & SicherheitsmaÃŸnahmen**
âœ… **Einfache Konfiguration Ã¼ber `config.yaml` & `.env`**

ğŸš€ **WebCrawler-Pro ist die ideale LÃ¶sung fÃ¼r produktives, sicheres und flexibles Web-Scraping!**

